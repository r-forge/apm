
R version 3.0.0 RC (2013-03-27 r62426) -- "Masked Marvel"
Copyright (C) 2013 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin10.8.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ################################################################################
> ### R code from Applied Predictive Modeling (2013) by Kuhn and Johnson.
> ### Copyright 2013 Kuhn and Johnson
> ### Web Page: http://www.appliedpredictivemodeling.com
> ### Contact: Max Kuhn (mxkuhn@gmail.com) 
> ###
> ### Chapter 17: Case Study: Job Scheduling
> ###
> ### Required packages: AppliedPredictiveModeling, C50, caret, doMC (optional),
> ###                    earth, Hmisc, ipred, tabplot, kernlab, lattice, MASS,
> ###                    mda, nnet, pls, randomForest, rpart, sparseLDA, 
> ###
> ### Data used: The HPC job scheduling data in the AppliedPredictiveModeling
> ###            package.
> ###
> ### Notes: 
> ### 1) This code is provided without warranty.
> ###
> ### 2) This code should help the user reproduce the results in the
> ### text. There will be differences between this code and what is is
> ### the computing section. For example, the computing sections show
> ### how the source functions work (e.g. randomForest() or plsr()),
> ### which were not directly used when creating the book. Also, there may be 
> ### syntax differences that occur over time as packages evolve. These files 
> ### will reflect those changes.
> ###
> ### 3) In some cases, the calculations in the book were run in 
> ### parallel. The sub-processes may reset the random number seed.
> ### Your results may slightly vary.
> ###
> ################################################################################
> 
> library(AppliedPredictiveModeling)
Loading required package: CORElearn
Loading required package: cluster
Loading required package: rpart
Loading required package: MASS
Loading required package: plyr
Loading required package: reshape2
> data(schedulingData)
> 
> ### Make a vector of predictor names
> predictors <- names(schedulingData)[!(names(schedulingData) %in% c("Class"))]
> 
> ### A few summaries and plots of the data
> library(Hmisc)
Loading required package: survival
Loading required package: splines
Hmisc library by Frank E Harrell Jr

Type library(help='Hmisc'), ?Overview, or ?Hmisc.Overview')
to see overall documentation.

NOTE:Hmisc no longer redefines [.factor to drop unused levels when
subsetting.  To get the old behavior of Hmisc type dropUnusedLevels().


Attaching package: ‘Hmisc’

The following object is masked from ‘package:survival’:

    untangle.specials

The following object is masked from ‘package:plyr’:

    is.discrete, summarize

The following object is masked from ‘package:base’:

    format.pval, round.POSIXt, trunc.POSIXt, units

> describe(schedulingData)
schedulingData 

 8  Variables      4331  Observations
--------------------------------------------------------------------------------
Protocol 
      n missing  unique 
   4331       0      14 

           A   C   D  E   F   G   H   I   J K   L   M   N   O
Frequency 94 160 149 96 170 155 321 381 989 6 242 451 536 581
%          2   4   3  2   4   4   7   9  23 0   6  10  12  13
--------------------------------------------------------------------------------
Compounds 
      n missing  unique    Mean     .05     .10     .25     .50     .75     .90 
   4331       0     858   497.7      27      37      98     226     448     967 
    .95 
   2512 

lowest :    20    21    22    23    24, highest: 14087 14090 14091 14097 14103 
--------------------------------------------------------------------------------
InputFields 
      n missing  unique    Mean     .05     .10     .25     .50     .75     .90 
   4331       0    1730    1537      26      48     134     426     991    4165 
    .95 
   7594 

lowest :    10    11    12    13    14, highest: 36021 45420 45628 55920 56671 
--------------------------------------------------------------------------------
Iterations 
      n missing  unique    Mean     .05     .10     .25     .50     .75     .90 
   4331       0      11   29.24      10      20      20      20      20      50 
    .95 
    100 

           10 11 15   20 30 40  50 100 125 150 200
Frequency 272  9  2 3568  3  7 153 188   1   2 126
%           6  0  0   82  0  0   4   4   0   0   3
--------------------------------------------------------------------------------
NumPending 
      n missing  unique    Mean     .05     .10     .25     .50     .75     .90 
   4331       0     303   53.39     0.0     0.0     0.0     0.0     0.0    33.0 
    .95 
  145.5 

lowest :    0    1    2    3    4, highest: 3822 3870 3878 5547 5605 
--------------------------------------------------------------------------------
Hour 
      n missing  unique    Mean     .05     .10     .25     .50     .75     .90 
   4331       0     924   13.73   7.025   9.333  10.900  14.017  16.600  18.250 
    .95 
 19.658 

lowest :  0.01667  0.03333  0.08333  0.10000  0.11667
highest: 23.20000 23.21667 23.35000 23.80000 23.98333 
--------------------------------------------------------------------------------
Day 
      n missing  unique 
   4331       0       7 

          Mon Tue Wed Thu Fri Sat Sun
Frequency 692 900 903 720 923  32 161
%          16  21  21  17  21   1   4
--------------------------------------------------------------------------------
Class 
      n missing  unique 
   4331       0       4 

VF (2211, 51%), F (1347, 31%), M (514, 12%), L (259, 6%) 
--------------------------------------------------------------------------------
> 
> library(tabplot)
Loading required package: ffbase
Loading required package: ff
Loading required package: tools
Loading required package: bit
Attaching package bit
package:bit (c) 2008-2012 Jens Oehlschlaegel (GPL-2)
creators: bit bitwhich
coercion: as.logical as.integer as.bit as.bitwhich which
operator: ! & | xor != ==
querying: print length any all min max range sum summary
bit access: length<- [ [<- [[ [[<-
for more help type ?bit

Attaching package: ‘bit’

The following object is masked from ‘package:base’:

    xor

Attaching package ff
- getOption("fftempdir")=="/var/folders/ll/1hd6yrrn0lgfxn3xhq0qmp_w0000gn/T//RtmpoOCNMD"

- getOption("ffextension")=="ff"

- getOption("ffdrop")==TRUE

- getOption("fffinonexit")==TRUE

- getOption("ffpagesize")==65536

- getOption("ffcaching")=="mmnoflush"  -- consider "ffeachflush" if your system stalls on large writes

- getOption("ffbatchbytes")==16777216 -- consider a different value for tuning your system

- getOption("ffmaxbytes")==536870912 -- consider a different value for tuning your system


Attaching package: ‘ff’

The following object is masked from ‘package:utils’:

    write.csv, write.csv2

The following object is masked from ‘package:base’:

    is.factor, is.ordered


Attaching package: ‘ffbase’

The following object is masked from ‘package:base’:

    %in%

Loading required package: grid
> tableplot(schedulingData[, c( "Class", predictors)])
> 
> mosaicplot(table(schedulingData$Protocol, 
+                  schedulingData$Class), 
+            main = "")
> 
> library(lattice)
> xyplot(Compounds ~ InputFields|Protocol,
+        data = schedulingData,
+        scales = list(x = list(log = 10), y = list(log = 10)),
+        groups = Class,
+        xlab = "Input Fields",
+        auto.key = list(columns = 4),
+        aspect = 1,
+        as.table = TRUE)
> 
> 
> ################################################################################
> ### Section 17.1 Data Splitting and Model Strategy
> 
> ## Split the data
> 
> library(caret)
Loading required package: foreach
> set.seed(1104)
> inTrain <- createDataPartition(schedulingData$Class, p = .8, list = FALSE)
> 
> ### There are a lot of zeros and the distribution is skewed. We add
> ### one so that we can log transform the data
> schedulingData$NumPending <- schedulingData$NumPending + 1
> 
> trainData <- schedulingData[ inTrain,]
> testData  <- schedulingData[-inTrain,]
> 
> ### Create a main effects only model formula to use
> ### repeatedly. Another formula with nonlinear effects is created
> ### below.
> modForm <- as.formula(Class ~ Protocol + log10(Compounds) +
+   log10(InputFields)+ log10(Iterations) +
+   log10(NumPending) + Hour + Day)
> 
> ### Create an expanded set of predictors with interactions. 
> 
> modForm2 <- as.formula(Class ~ (Protocol + log10(Compounds) +
+   log10(InputFields)+ log10(Iterations) +
+   log10(NumPending) + Hour + Day)^2)
> 
> 
> ### Some of these terms will not be estimable. For example, if there
> ### are no data points were a particular protocol was run on a
> ### particular day, the full interaction cannot be computed. We use
> ### model.matrix() to create the whole set of predictor columns, then
> ### remove those that are zero variance
> 
> expandedTrain <- model.matrix(modForm2, data = trainData)
> expandedTest  <- model.matrix(modForm2, data = testData)
> expandedTrain <- as.data.frame(expandedTrain)
> expandedTest  <-  as.data.frame(expandedTest)
> 
> ### Some models have issues when there is a zero variance predictor
> ### within the data of a particular class, so we used caret's
> ### checkConditionalX() function to find the offending columns and
> ### remove them
> 
> zv <- checkConditionalX(expandedTrain, trainData$Class)
> 
> ### Keep the expanded set to use for models where we must manually add
> ### more complex terms (such as logistic regression)
> 
> expandedTrain <-  expandedTrain[,-zv]
> expandedTest  <-  expandedTest[, -zv]
> 
> ### Create the cost matrix
> costMatrix <- ifelse(diag(4) == 1, 0, 1)
> costMatrix[4, 1] <- 10
> costMatrix[3, 1] <- 5
> costMatrix[4, 2] <- 5
> costMatrix[3, 2] <- 5
> rownames(costMatrix) <- colnames(costMatrix) <- levels(trainData$Class)
> 
> ### Create a cost function
> cost <- function(pred, obs)
+ {
+   isNA <- is.na(pred)
+   if(!all(isNA))
+   {
+     pred <- pred[!isNA]
+     obs <- obs[!isNA]
+     
+     cost <- ifelse(pred == obs, 0, 1)
+     if(any(pred == "VF" & obs == "L")) cost[pred == "L" & obs == "VF"] <- 10
+     if(any(pred == "F" & obs == "L")) cost[pred == "F" & obs == "L"] <- 5
+     if(any(pred == "F" & obs == "M")) cost[pred == "F" & obs == "M"] <- 5
+     if(any(pred == "VF" & obs == "M")) cost[pred == "VF" & obs == "M"] <- 5
+     out <- mean(cost)
+   } else out <- NA
+   out
+ }
> 
> ### Make a summary function that can be used with caret's train() function
> costSummary <- function (data, lev = NULL, model = NULL)
+ {
+   if (is.character(data$obs))  data$obs <- factor(data$obs, levels = lev)
+   c(postResample(data[, "pred"], data[, "obs"]),
+     Cost = cost(data[, "pred"], data[, "obs"]))
+ }
> 
> ### Create a contrl object for the models
> ctrl <- trainControl(method = "repeatedcv", 
+                      repeats = 5,
+                      summaryFunction = costSummary)
> 
> ### Optional: parallel processing can be used via the 'do' packages,
> ### such as doMC, doMPI etc. We used doMC (not on Windows) to speed
> ### up the computations.
> 
> ### WARNING: Be aware of how much memory is needed to parallel
> ### process. It can very quickly overwhelm the available hardware. The
> ### estimate of the median memory usage (VSIZE = total memory size) 
> ### was 3300-4100M per core although the some calculations require as  
> ### much as 3400M without parallel processing. 
> 
> library(doMC)
Loading required package: iterators
Loading required package: parallel
> registerDoMC(14)
> 
> ### Fit the CART model with and without costs
> 
> set.seed(857)
> rpFit <- train(x = trainData[, predictors],
+                y = trainData$Class,
+                method = "rpart",
+                metric = "Cost",
+                maximize = FALSE,
+                tuneLength = 20,
+                trControl = ctrl)
Loading required package: class

Attaching package: ‘e1071’

The following object is masked from ‘package:Hmisc’:

    impute

> rpFit
3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

No pre-processing
Resampling: Cross-Validation (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  cp       Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  0.00236  0.774     0.631  0.51   0.0193       0.0323    0.0617 
  0.00249  0.773     0.63   0.514  0.0193       0.0319    0.0591 
  0.00294  0.768     0.621  0.537  0.0176       0.0305    0.0514 
  0.00324  0.766     0.617  0.542  0.0169       0.0298    0.0521 
  0.00353  0.764     0.611  0.55   0.017        0.03      0.0491 
  0.00383  0.762     0.607  0.56   0.0182       0.0321    0.0538 
  0.00471  0.76      0.603  0.569  0.0193       0.0345    0.0607 
  0.0053   0.758     0.597  0.58   0.0183       0.0326    0.0567 
  0.00589  0.756     0.594  0.585  0.0201       0.0355    0.0591 
  0.00648  0.751     0.586  0.604  0.0205       0.036     0.059  
  0.00824  0.735     0.558  0.647  0.0184       0.0327    0.0491 
  0.00942  0.727     0.544  0.663  0.0184       0.0328    0.0476 
  0.00982  0.723     0.539  0.667  0.0181       0.0325    0.047  
  0.01     0.719     0.532  0.67   0.0175       0.0317    0.0454 
  0.0159   0.703     0.505  0.697  0.0192       0.0327    0.0518 
  0.0171   0.698     0.495  0.717  0.0179       0.032     0.0586 
  0.0183   0.693     0.482  0.755  0.0208       0.0409    0.0797 
  0.0205   0.67      0.42   0.871  0.0227       0.0493    0.0626 
  0.0383   0.652     0.376  0.969  0.0177       0.0346    0.0517 
  0.274    0.568     0.159  0.992  0.0609       0.168     0.0323 

Cost was used to select the optimal model using  the smallest value.
The final value used for the model was cp = 0.00236. 
> 
> set.seed(857)
> rpFitCost <- train(x = trainData[, predictors],
+                    y = trainData$Class,
+                    method = "rpart",
+                    metric = "Cost",
+                    maximize = FALSE,
+                    tuneLength = 20,
+                    parms =list(loss = costMatrix),
+                    trControl = ctrl)
> rpFitCost
3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

No pre-processing
Resampling: Cross-Validation (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  cp       Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  0.00236  0.72      0.565  0.343  0.0161       0.0248    0.0325 
  0.00249  0.718     0.562  0.344  0.0162       0.0248    0.0336 
  0.00294  0.717     0.56   0.344  0.0186       0.0277    0.0349 
  0.00324  0.717     0.56   0.345  0.0182       0.0272    0.0344 
  0.00353  0.713     0.555  0.35   0.0197       0.0293    0.0362 
  0.00383  0.707     0.545  0.358  0.0201       0.0297    0.038  
  0.00471  0.699     0.533  0.366  0.0205       0.0297    0.0386 
  0.0053   0.685     0.513  0.381  0.0196       0.0281    0.0376 
  0.00589  0.675     0.501  0.392  0.0207       0.0288    0.0378 
  0.00648  0.656     0.479  0.403  0.0372       0.0482    0.0461 
  0.00824  0.63      0.449  0.428  0.0451       0.0555    0.0476 
  0.00942  0.623     0.44   0.436  0.0574       0.0687    0.0478 
  0.00982  0.62      0.436  0.443  0.0581       0.0697    0.0457 
  0.01     0.617     0.433  0.445  0.0583       0.0699    0.0436 
  0.0159   0.53      0.324  0.507  0.0257       0.0303    0.0312 
  0.0171   0.52      0.306  0.526  0.0201       0.0223    0.0276 
  0.0183   0.521     0.305  0.527  0.0194       0.0219    0.0277 
  0.0205   0.515     0.295  0.532  0.0187       0.0231    0.0299 
  0.0383   0.503     0.275  0.546  0.0161       0.0179    0.0269 
  0.274    0.119     0      0.881  0.00104      0         0.00104

Cost was used to select the optimal model using  the smallest value.
The final value used for the model was cp = 0.00236. 
> 
> set.seed(857)
> ldaFit <- train(x = expandedTrain,
+                 y = trainData$Class,
+                 method = "lda",
+                 metric = "Cost",
+                 maximize = FALSE,
+                 trControl = ctrl)
> ldaFit
3467 samples
 112 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

No pre-processing
Resampling: Cross-Validation (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results

  Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  0.756     0.602  0.523  0.0232       0.0389    0.0495 

 
> 
> sldaGrid <- expand.grid(.NumVars = seq(2, 112, by = 5),
+                         .lambda = c(0, 0.01, .1, 1, 10))
> set.seed(857)
> sldaFit <- train(x = expandedTrain,
+                  y = trainData$Class,
+                  method = "sparseLDA",
+                  tuneGrid = sldaGrid,
+                  preProc = c("center", "scale"),
+                  metric = "Cost",
+                  maximize = FALSE,
+                  trControl = ctrl)
Loading required package: lars
Loading required package: lars
Loading required package: lars
Loaded lars 1.1

Loaded lars 1.1

Loading required package: lars
Loading required package: elasticnet
Loaded lars 1.1

Loading required package: lars
Loading required package: elasticnet
Loading required package: elasticnet
Loaded lars 1.1

Loading required package: lars
Loaded lars 1.1

Loading required package: elasticnet
Loading required package: mda
Loading required package: lars
Loading required package: lars
Loading required package: lars
Loading required package: mda
Loading required package: elasticnet
Loading required package: lars
Loaded lars 1.1

Loaded lars 1.1

Loading required package: lars
Loading required package: mda
Loading required package: lars
Loaded lars 1.1

Loading required package: mda
Loaded lars 1.1

Loaded lars 1.1

Loading required package: elasticnet
Loading required package: elasticnet
Loading required package: mda
Loading required package: lars
Loaded lars 1.1

Loaded lars 1.1

Loading required package: elasticnet
Loading required package: elasticnet
Loading required package: elasticnet
Loading required package: lars
Loading required package: elasticnet
Loading required package: elasticnet
Loading required package: mda
Loaded lars 1.1

Loading required package: mda
Loading required package: mda
Loading required package: mda
Loading required package: mda
Loaded lars 1.1

Loading required package: elasticnet
Loading required package: mda
Loading required package: elasticnet
Loading required package: mda
Loading required package: mda
Loading required package: mda
Loading required package: lars
Loaded lars 1.1

Loading required package: elasticnet
Loading required package: mda
> sldaFit
3467 samples
 112 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

Pre-processing: centered, scaled 
Resampling: Cross-Validation (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  NumVars  lambda  Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  2        0       0.664     0.418  0.69   0.0164       0.0302    0.0566 
  2        0.01    0.662     0.416  0.692  0.018        0.033     0.0575 
  2        0.1     0.663     0.417  0.691  0.0175       0.0318    0.0584 
  2        1       0.663     0.418  0.691  0.0166       0.0316    0.0566 
  2        10      0.664     0.418  0.689  0.0158       0.0297    0.0545 
  7        0       0.681     0.457  0.707  0.0187       0.0333    0.0512 
  7        0.01    0.681     0.457  0.707  0.0187       0.0332    0.0511 
  7        0.1     0.681     0.457  0.707  0.0187       0.0333    0.0512 
  7        1       0.681     0.457  0.707  0.0188       0.0334    0.0512 
  7        10      0.681     0.457  0.707  0.0193       0.0341    0.0503 
  12       0       0.688     0.47   0.687  0.0181       0.0324    0.0526 
  12       0.01    0.688     0.47   0.687  0.0181       0.0324    0.0526 
  12       0.1     0.688     0.471  0.686  0.0182       0.0325    0.0524 
  12       1       0.688     0.47   0.687  0.018        0.0323    0.0522 
  12       10      0.687     0.469  0.689  0.0182       0.0325    0.0515 
  17       0       0.694     0.483  0.661  0.018        0.0318    0.0517 
  17       0.01    0.694     0.483  0.66   0.0178       0.0316    0.0511 
  17       0.1     0.694     0.482  0.661  0.0179       0.0316    0.0517 
  17       1       0.694     0.483  0.661  0.0177       0.0315    0.0514 
  17       10      0.694     0.482  0.661  0.0178       0.0316    0.0493 
  22       0       0.699     0.493  0.651  0.0187       0.0323    0.0487 
  22       0.01    0.699     0.493  0.651  0.0187       0.0323    0.0487 
  22       0.1     0.699     0.493  0.651  0.0187       0.0323    0.0487 
  22       1       0.699     0.493  0.651  0.0187       0.0323    0.049  
  22       10      0.697     0.491  0.652  0.0185       0.032     0.0501 
  27       0       0.704     0.503  0.638  0.0193       0.0337    0.0574 
  27       0.01    0.704     0.502  0.637  0.0195       0.0343    0.0581 
  27       0.1     0.704     0.502  0.638  0.0195       0.0342    0.0581 
  27       1       0.704     0.503  0.637  0.0194       0.034     0.058  
  27       10      0.703     0.501  0.636  0.0198       0.0345    0.0593 
  32       0       0.712     0.518  0.625  0.0191       0.0336    0.0572 
  32       0.01    0.712     0.518  0.626  0.0191       0.0337    0.0573 
  32       0.1     0.712     0.518  0.626  0.0191       0.0335    0.0567 
  32       1       0.712     0.518  0.626  0.0191       0.0334    0.0568 
  32       10      0.71      0.514  0.628  0.019        0.0331    0.0556 
  37       0       0.721     0.536  0.611  0.0187       0.0322    0.0538 
  37       0.01    0.721     0.536  0.611  0.0187       0.0322    0.0538 
  37       0.1     0.721     0.536  0.611  0.0187       0.0322    0.0538 
  37       1       0.721     0.536  0.611  0.0188       0.0323    0.0533 
  37       10      0.717     0.529  0.615  0.0197       0.034     0.0575 
  42       0       0.725     0.544  0.596  0.0186       0.0314    0.0508 
  42       0.01    0.725     0.544  0.596  0.0186       0.0315    0.0506 
  42       0.1     0.725     0.544  0.596  0.0185       0.0312    0.0505 
  42       1       0.725     0.544  0.595  0.0183       0.031     0.0521 
  42       10      0.723     0.541  0.598  0.0203       0.0344    0.0522 
  47       0       0.727     0.548  0.579  0.0193       0.0322    0.0486 
  47       0.01    0.727     0.548  0.578  0.0193       0.0321    0.048  
  47       0.1     0.727     0.548  0.578  0.0193       0.0322    0.0483 
  47       1       0.727     0.548  0.579  0.0193       0.0323    0.049  
  47       10      0.725     0.546  0.584  0.0204       0.0338    0.0515 
  52       0       0.727     0.549  0.577  0.0206       0.0344    0.0476 
  52       0.01    0.727     0.548  0.578  0.0205       0.0343    0.0475 
  52       0.1     0.727     0.549  0.577  0.0205       0.0342    0.0475 
  52       1       0.727     0.548  0.577  0.021        0.0351    0.0483 
  52       10      0.725     0.546  0.579  0.0205       0.034     0.0495 
  57       0       0.729     0.553  0.573  0.021        0.0351    0.0463 
  57       0.01    0.729     0.553  0.573  0.0209       0.0349    0.0463 
  57       0.1     0.729     0.553  0.573  0.0208       0.0348    0.0462 
  57       1       0.729     0.553  0.573  0.021        0.035     0.0455 
  57       10      0.728     0.551  0.574  0.0211       0.035     0.048  
  62       0       0.736     0.565  0.56   0.0215       0.0358    0.0475 
  62       0.01    0.736     0.565  0.56   0.0215       0.0358    0.0475 
  62       0.1     0.736     0.565  0.56   0.0214       0.0358    0.0475 
  62       1       0.736     0.565  0.56   0.0211       0.0352    0.0475 
  62       10      0.733     0.56   0.563  0.021        0.0351    0.0485 
  67       0       0.742     0.576  0.549  0.0208       0.0344    0.0431 
  67       0.01    0.742     0.576  0.549  0.0208       0.0345    0.0432 
  67       0.1     0.743     0.576  0.549  0.0208       0.0345    0.0432 
  67       1       0.743     0.577  0.547  0.0211       0.035     0.0448 
  67       10      0.739     0.57   0.553  0.0204       0.0339    0.0452 
  72       0       0.747     0.585  0.539  0.0207       0.0346    0.0456 
  72       0.01    0.747     0.585  0.539  0.0207       0.0346    0.0456 
  72       0.1     0.747     0.585  0.539  0.0206       0.0344    0.0454 
  72       1       0.747     0.584  0.54   0.0205       0.0343    0.0447 
  72       10      0.743     0.577  0.546  0.0205       0.0342    0.0436 
  77       0       0.751     0.591  0.534  0.0207       0.0347    0.042  
  77       0.01    0.751     0.591  0.534  0.0207       0.0347    0.042  
  77       0.1     0.751     0.591  0.534  0.0208       0.0348    0.0421 
  77       1       0.75      0.589  0.535  0.0213       0.0358    0.0429 
  77       10      0.747     0.584  0.54   0.0207       0.0345    0.0424 
  82       0       0.753     0.595  0.529  0.0196       0.0326    0.0409 
  82       0.01    0.753     0.595  0.529  0.0196       0.0326    0.041  
  82       0.1     0.753     0.595  0.529  0.0196       0.0326    0.0404 
  82       1       0.753     0.594  0.53   0.0199       0.0331    0.0399 
  82       10      0.748     0.586  0.537  0.0215       0.0359    0.0418 
  87       0       0.755     0.598  0.526  0.0202       0.0337    0.0429 
  87       0.01    0.755     0.598  0.526  0.0202       0.0337    0.0428 
  87       0.1     0.755     0.598  0.525  0.0203       0.0339    0.043  
  87       1       0.755     0.598  0.526  0.0204       0.0339    0.0414 
  87       10      0.75      0.59   0.532  0.0207       0.0347    0.0404 
  92       0       0.754     0.598  0.526  0.0214       0.0356    0.0452 
  92       0.01    0.754     0.598  0.527  0.0215       0.0358    0.0451 
  92       0.1     0.755     0.598  0.526  0.0216       0.036     0.0452 
  92       1       0.754     0.598  0.526  0.021        0.035     0.0453 
  92       10      0.752     0.593  0.531  0.0213       0.0357    0.044  
  97       0       0.755     0.599  0.526  0.0217       0.0361    0.0452 
  97       0.01    0.755     0.599  0.526  0.0217       0.0362    0.0452 
  97       0.1     0.755     0.599  0.525  0.0218       0.0364    0.0457 
  97       1       0.755     0.599  0.525  0.0219       0.0363    0.0457 
  97       10      0.752     0.594  0.53   0.0218       0.0364    0.0444 
  102      0       0.754     0.598  0.527  0.0226       0.0377    0.0469 
  102      0.01    0.754     0.598  0.527  0.0224       0.0374    0.0467 
  102      0.1     0.754     0.598  0.527  0.0223       0.0373    0.0472 
  102      1       0.755     0.599  0.527  0.0224       0.0373    0.0475 
  102      10      0.753     0.595  0.53   0.0222       0.037     0.0458 
  107      0       0.755     0.6    0.526  0.0233       0.0388    0.0497 
  107      0.01    0.755     0.6    0.526  0.0232       0.0387    0.0497 
  107      0.1     0.755     0.6    0.527  0.023        0.0384    0.0495 
  107      1       0.755     0.6    0.527  0.0226       0.0377    0.0479 
  107      10      0.753     0.597  0.53   0.0227       0.0378    0.0472 
  112      0       0.756     0.602  0.523  0.0232       0.0389    0.0495 
  112      0.01    0.756     0.602  0.523  0.0232       0.0388    0.0493 
  112      0.1     0.756     0.602  0.523  0.0232       0.0387    0.0501 
  112      1       0.756     0.601  0.524  0.0234       0.0391    0.0503 
  112      10      0.754     0.597  0.53   0.023        0.0384    0.0494 

Cost was used to select the optimal model using  the smallest value.
The final values used for the model were NumVars = 112 and lambda = 0. 
> 
> set.seed(857)
> nnetGrid <- expand.grid(.decay = c(0, 0.001, 0.01, .1, .5),
+                         .size = (1:10)*2 - 1)
> nnetFit <- train(modForm, 
+                  data = trainData,
+                  method = "nnet",
+                  metric = "Cost",
+                  maximize = FALSE,
+                  tuneGrid = nnetGrid,
+                  trace = FALSE,
+                  MaxNWts = 2000,
+                  maxit = 1000,
+                  preProc = c("center", "scale"),
+                  trControl = ctrl)
> nnetFit
3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

Pre-processing: centered, scaled 
Resampling: Cross-Validation (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  size  decay  Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  1     0      0.682     0.461  0.878  0.0263       0.046     0.16   
  1     0.001  0.695     0.489  0.76   0.026        0.0456    0.103  
  1     0.01   0.694     0.488  0.74   0.022        0.0375    0.0521 
  1     0.1    0.694     0.486  0.765  0.0201       0.0341    0.048  
  1     0.5    0.708     0.501  0.846  0.0206       0.0364    0.0555 
  3     0      0.747     0.583  0.609  0.0275       0.0456    0.0956 
  3     0.001  0.752     0.592  0.589  0.0251       0.0424    0.0655 
  3     0.01   0.756     0.599  0.591  0.0214       0.0363    0.0639 
  3     0.1    0.755     0.598  0.596  0.0223       0.0364    0.0636 
  3     0.5    0.756     0.598  0.595  0.0181       0.031     0.0604 
  5     0      0.762     0.613  0.529  0.0255       0.042     0.0899 
  5     0.001  0.765     0.616  0.517  0.0247       0.0418    0.0789 
  5     0.01   0.769     0.624  0.521  0.0222       0.0373    0.0665 
  5     0.1    0.778     0.639  0.501  0.0204       0.0332    0.052  
  5     0.5    0.777     0.636  0.521  0.0187       0.0316    0.0547 
  7     0      0.772     0.629  0.489  0.0222       0.0357    0.0619 
  7     0.001  0.776     0.637  0.493  0.0229       0.0371    0.0677 
  7     0.01   0.781     0.645  0.477  0.0183       0.0305    0.057  
  7     0.1    0.788     0.656  0.463  0.0202       0.0334    0.0563 
  7     0.5    0.787     0.652  0.484  0.0197       0.033     0.0522 
  9     0      0.77      0.628  0.466  0.0237       0.0381    0.0569 
  9     0.001  0.78      0.644  0.457  0.0216       0.0355    0.0537 
  9     0.01   0.781     0.645  0.463  0.0232       0.0377    0.058  
  9     0.1    0.79      0.659  0.452  0.0194       0.0322    0.0505 
  9     0.5    0.786     0.652  0.469  0.0214       0.0348    0.0496 
  11    0      0.776     0.637  0.457  0.0206       0.0338    0.0638 
  11    0.001  0.779     0.643  0.451  0.0205       0.0336    0.0493 
  11    0.01   0.786     0.655  0.432  0.0199       0.032     0.0451 
  11    0.1    0.794     0.667  0.427  0.0191       0.0308    0.0467 
  11    0.5    0.793     0.664  0.451  0.0199       0.0325    0.0431 
  13    0      0.776     0.638  0.451  0.0233       0.0381    0.0516 
  13    0.001  0.777     0.641  0.44   0.0193       0.0308    0.0472 
  13    0.01   0.786     0.655  0.424  0.0198       0.0322    0.0484 
  13    0.1    0.797     0.672  0.419  0.0187       0.0303    0.043  
  13    0.5    0.797     0.671  0.434  0.0175       0.0283    0.0382 
  15    0      0.769     0.628  0.448  0.0211       0.0339    0.0457 
  15    0.001  0.776     0.639  0.443  0.0198       0.0317    0.0529 
  15    0.01   0.789     0.659  0.42   0.0183       0.0297    0.0459 
  15    0.1    0.796     0.67   0.418  0.0197       0.0317    0.0446 
  15    0.5    0.798     0.672  0.435  0.024        0.0388    0.0526 
  17    0      0.774     0.638  0.437  0.019        0.0305    0.0494 
  17    0.001  0.778     0.643  0.426  0.0215       0.0335    0.04   
  17    0.01   0.789     0.661  0.406  0.0183       0.0298    0.0466 
  17    0.1    0.8       0.678  0.403  0.0187       0.0312    0.0452 
  17    0.5    0.797     0.671  0.43   0.0191       0.031     0.0477 
  19    0      0.768     0.629  0.435  0.0215       0.0339    0.0513 
  19    0.001  0.779     0.647  0.422  0.0249       0.0394    0.057  
  19    0.01   0.791     0.664  0.404  0.0212       0.0342    0.0464 
  19    0.1    0.797     0.673  0.402  0.0226       0.0362    0.0468 
  19    0.5    0.803     0.68   0.417  0.0194       0.0317    0.0442 

Cost was used to select the optimal model using  the smallest value.
The final values used for the model were size = 19 and decay = 0.1. 
> 
> set.seed(857)
> plsFit <- train(x = expandedTrain,
+                 y = trainData$Class,
+                 method = "pls",
+                 metric = "Cost",
+                 maximize = FALSE,
+                 tuneLength = 100,
+                 preProc = c("center", "scale"),
+                 trControl = ctrl)

Attaching package: ‘pls’

The following object is masked from ‘package:caret’:

    R2

The following object is masked from ‘package:stats’:

    loadings


Attaching package: ‘pls’

The following object is masked from ‘package:caret’:

    R2


Attaching package: ‘pls’

The following object is masked from ‘package:caret’:

    R2

The following object is masked from ‘package:stats’:

    loadings

The following object is masked from ‘package:stats’:

    loadings


Attaching package: ‘pls’

The following object is masked from ‘package:caret’:

    R2

The following object is masked from ‘package:stats’:

    loadings


Attaching package: ‘pls’

The following object is masked from ‘package:caret’:

    R2

The following object is masked from ‘package:stats’:

    loadings


Attaching package: ‘pls’

The following object is masked from ‘package:caret’:

    R2


Attaching package: ‘pls’

The following object is masked from ‘package:caret’:

    R2

The following object is masked from ‘package:stats’:

    loadings

The following object is masked from ‘package:stats’:

    loadings


Attaching package: ‘pls’

The following object is masked from ‘package:caret’:

    R2

The following object is masked from ‘package:stats’:

    loadings


Attaching package: ‘pls’

The following object is masked from ‘package:caret’:

    R2

The following object is masked from ‘package:stats’:

    loadings


Attaching package: ‘pls’

The following object is masked from ‘package:caret’:

    R2

The following object is masked from ‘package:stats’:

    loadings


Attaching package: ‘pls’

The following object is masked from ‘package:caret’:

    R2

The following object is masked from ‘package:stats’:

    loadings


Attaching package: ‘pls’

The following object is masked from ‘package:caret’:

    R2

The following object is masked from ‘package:stats’:

    loadings


Attaching package: ‘pls’

The following object is masked from ‘package:caret’:

    R2

The following object is masked from ‘package:stats’:

    loadings


Attaching package: ‘pls’

The following object is masked from ‘package:caret’:

    R2

The following object is masked from ‘package:stats’:

    loadings


Attaching package: ‘pls’

The following object is masked from ‘package:caret’:

    R2

The following object is masked from ‘package:stats’:

    loadings

> plsFit
3467 samples
 112 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

Pre-processing: centered, scaled 
Resampling: Cross-Validation (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  ncomp  Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  1      0.645     0.352  0.998  0.0172       0.0342    0.0282 
  2      0.638     0.342  1.03   0.016        0.031     0.0264 
  3      0.646     0.357  1.02   0.0158       0.0311    0.0244 
  4      0.649     0.369  0.974  0.0162       0.0316    0.0408 
  5      0.662     0.4    0.921  0.0169       0.0319    0.0365 
  6      0.676     0.43   0.878  0.0195       0.0359    0.0485 
  7      0.677     0.434  0.853  0.0197       0.0363    0.0499 
  8      0.682     0.445  0.828  0.0203       0.0376    0.0532 
  9      0.689     0.457  0.796  0.0194       0.0358    0.0483 
  10     0.691     0.463  0.788  0.0194       0.0361    0.0515 
  11     0.692     0.467  0.776  0.0202       0.037     0.046  
  12     0.698     0.479  0.768  0.0196       0.0356    0.0496 
  13     0.7       0.484  0.761  0.0196       0.0352    0.0487 
  14     0.701     0.485  0.768  0.0196       0.0347    0.0493 
  15     0.701     0.486  0.766  0.0201       0.0362    0.051  
  16     0.704     0.492  0.761  0.0208       0.037     0.0504 
  17     0.707     0.497  0.761  0.0209       0.0376    0.0496 
  18     0.706     0.496  0.759  0.0194       0.0347    0.0527 
  19     0.707     0.498  0.756  0.0212       0.0376    0.0543 
  20     0.71      0.503  0.75   0.0186       0.0332    0.0486 
  21     0.716     0.514  0.74   0.0196       0.0347    0.052  
  22     0.719     0.519  0.734  0.0193       0.0344    0.0512 
  23     0.729     0.537  0.725  0.0184       0.0324    0.0485 
  24     0.726     0.533  0.731  0.0202       0.0355    0.0512 
  25     0.727     0.536  0.712  0.0198       0.0349    0.0489 
  26     0.727     0.536  0.711  0.0218       0.0381    0.0495 
  27     0.728     0.539  0.708  0.0205       0.0363    0.0495 
  28     0.728     0.539  0.703  0.0205       0.0361    0.0525 
  29     0.728     0.54   0.704  0.021        0.037     0.0514 
  30     0.73      0.543  0.698  0.0215       0.0378    0.0515 
  31     0.731     0.546  0.695  0.0213       0.0373    0.0499 
  32     0.732     0.547  0.693  0.0225       0.0393    0.0497 
  33     0.734     0.551  0.688  0.0216       0.0378    0.0487 
  34     0.736     0.553  0.684  0.0216       0.0377    0.0497 
  35     0.737     0.556  0.683  0.0198       0.0348    0.0464 
  36     0.739     0.559  0.677  0.0202       0.0353    0.0469 
  37     0.74      0.56   0.675  0.0217       0.0378    0.0503 
  38     0.74      0.561  0.673  0.0199       0.0345    0.049  
  39     0.742     0.564  0.669  0.0203       0.0354    0.0509 
  40     0.741     0.563  0.67   0.019        0.0333    0.0491 
  41     0.742     0.564  0.667  0.0196       0.034     0.0492 
  42     0.742     0.564  0.666  0.0197       0.0342    0.0509 
  43     0.742     0.565  0.662  0.0203       0.0352    0.0507 
  44     0.743     0.567  0.661  0.0202       0.0349    0.0499 
  45     0.743     0.567  0.658  0.0203       0.0354    0.0501 
  46     0.743     0.568  0.657  0.0205       0.0356    0.0503 
  47     0.743     0.568  0.655  0.0203       0.0352    0.0494 
  48     0.745     0.571  0.65   0.02         0.0347    0.0497 
  49     0.744     0.57   0.652  0.0201       0.0349    0.0507 
  50     0.745     0.571  0.65   0.0199       0.0344    0.0491 
  51     0.744     0.569  0.652  0.0197       0.0339    0.0495 
  52     0.744     0.57   0.65   0.0197       0.0341    0.0494 
  53     0.745     0.571  0.649  0.0207       0.0357    0.0512 
  54     0.745     0.572  0.648  0.0204       0.0351    0.0499 
  55     0.745     0.572  0.648  0.0203       0.0349    0.0507 
  56     0.745     0.572  0.647  0.0196       0.0337    0.051  
  57     0.746     0.573  0.644  0.0194       0.0332    0.0481 
  58     0.745     0.572  0.646  0.0191       0.0328    0.0487 
  59     0.745     0.573  0.645  0.0197       0.034     0.05   
  60     0.746     0.573  0.644  0.0198       0.0342    0.0504 
  61     0.746     0.574  0.642  0.0194       0.0335    0.0495 
  62     0.746     0.574  0.641  0.0201       0.0347    0.0499 
  63     0.746     0.574  0.641  0.0206       0.0355    0.0505 
  64     0.747     0.575  0.641  0.0201       0.0347    0.05   
  65     0.747     0.575  0.64   0.0206       0.0354    0.0491 
  66     0.747     0.576  0.638  0.02         0.0345    0.0492 
  67     0.747     0.576  0.639  0.0203       0.0349    0.0488 
  68     0.747     0.576  0.639  0.0202       0.0347    0.0487 
  69     0.747     0.575  0.64   0.0204       0.0351    0.0502 
  70     0.747     0.576  0.639  0.0198       0.034     0.0491 
  71     0.747     0.576  0.638  0.0201       0.0345    0.0486 
  72     0.748     0.577  0.636  0.0201       0.0346    0.05   
  73     0.748     0.577  0.637  0.0201       0.0345    0.0496 
  74     0.748     0.577  0.637  0.0205       0.0354    0.0516 
  75     0.747     0.576  0.638  0.0207       0.0357    0.0523 
  76     0.747     0.576  0.639  0.0205       0.0353    0.0511 
  77     0.747     0.576  0.639  0.0201       0.0346    0.0501 
  78     0.747     0.576  0.639  0.02         0.0345    0.0506 
  79     0.747     0.575  0.639  0.0198       0.0341    0.0491 
  80     0.747     0.575  0.64   0.0197       0.034     0.0495 
  81     0.747     0.575  0.641  0.02         0.0344    0.0494 
  82     0.747     0.575  0.641  0.0203       0.035     0.0498 
  83     0.747     0.575  0.641  0.0201       0.0347    0.0494 
  84     0.747     0.575  0.641  0.0203       0.0349    0.0496 
  85     0.747     0.575  0.641  0.0203       0.035     0.0497 
  86     0.747     0.575  0.641  0.0198       0.0341    0.0494 
  87     0.747     0.575  0.641  0.0201       0.0346    0.0499 
  88     0.747     0.575  0.641  0.0202       0.0348    0.0499 
  89     0.747     0.575  0.641  0.0203       0.0349    0.0498 
  90     0.747     0.575  0.641  0.0203       0.035     0.0499 
  91     0.747     0.575  0.64   0.0204       0.0351    0.0501 
  92     0.747     0.575  0.641  0.0204       0.035     0.0498 
  93     0.747     0.575  0.641  0.0205       0.0353    0.0499 
  94     0.747     0.575  0.641  0.0206       0.0353    0.0499 
  95     0.747     0.575  0.641  0.0206       0.0354    0.0499 
  96     0.747     0.575  0.641  0.0205       0.0352    0.0498 
  97     0.747     0.575  0.641  0.0205       0.0352    0.0498 
  98     0.747     0.575  0.641  0.0205       0.0352    0.0498 
  99     0.747     0.575  0.641  0.0205       0.0352    0.0498 
  100    0.747     0.575  0.641  0.0206       0.0353    0.0499 

Cost was used to select the optimal model using  the smallest value.
The final value used for the model was ncomp = 72. 
> 
> set.seed(857)
> fdaFit <- train(modForm, data = trainData,
+                 method = "fda",
+                 metric = "Cost",
+                 maximize = FALSE,
+                 tuneLength = 25,
+                 trControl = ctrl)
Loading required package: leaps
Loading required package: plotmo
Loading required package: plotrix
> fdaFit
3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

No pre-processing
Resampling: Cross-Validation (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  nprune  Accuracy  Kappa   Cost   Accuracy SD  Kappa SD  Cost SD
  2       0.524     0.0711  0.929  0.00646      0.021     0.0455 
  3       0.541     0.142   0.843  0.00898      0.0221    0.0368 
  4       0.61      0.298   0.79   0.0143       0.03      0.0412 
  5       0.659     0.405   0.753  0.0156       0.03      0.042  
  6       0.678     0.451   0.75   0.018        0.0324    0.0468 
  7       0.684     0.466   0.699  0.0174       0.0305    0.0513 
  8       0.693     0.487   0.64   0.0206       0.0359    0.0522 
  9       0.695     0.491   0.634  0.0214       0.0369    0.0549 
  10      0.698     0.496   0.631  0.021        0.0363    0.0551 
  11      0.71      0.518   0.62   0.0224       0.0382    0.0575 
  12      0.713     0.524   0.617  0.0204       0.0351    0.054  
  13      0.715     0.529   0.612  0.0229       0.0388    0.0584 
  14      0.724     0.544   0.602  0.0222       0.0375    0.0593 
  15      0.726     0.547   0.602  0.019        0.0328    0.0567 
  16      0.727     0.548   0.602  0.0202       0.0344    0.0559 
  17      0.725     0.545   0.608  0.019        0.033     0.0571 
  18      0.726     0.547   0.606  0.0205       0.0352    0.0588 
  19      0.727     0.548   0.607  0.0206       0.0348    0.0598 
  20      0.727     0.549   0.606  0.0208       0.0353    0.0596 
  21      0.729     0.552   0.602  0.0213       0.0358    0.0572 
  22      0.731     0.555   0.6    0.0213       0.0361    0.0583 
  23      0.732     0.557   0.598  0.0202       0.0343    0.0562 

Tuning parameter 'degree' was held constant at a value of 1
Cost was used to select the optimal model using  the smallest value.
The final values used for the model were nprune = 23 and degree = 1. 
> 
> set.seed(857)
> rfFit <- train(x = trainData[, predictors],
+                y = trainData$Class,
+                method = "rf",
+                metric = "Cost",
+                maximize = FALSE,
+                tuneLength = 10,
+                ntree = 2000,
+                importance = TRUE,
+                trControl = ctrl)
randomForest 4.6-7
Type rfNews() to see new features/changes/bug fixes.

Attaching package: ‘randomForest’

The following object is masked from ‘package:Hmisc’:

    combine

note: only 6 unique complexity parameters in default grid. Truncating the grid to 6 .

> rfFit
3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

No pre-processing
Resampling: Cross-Validation (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  mtry  Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  2     0.842     0.743  0.334  0.0166       0.0271    0.0432 
  3     0.844     0.747  0.329  0.0168       0.0275    0.042  
  4     0.845     0.748  0.325  0.0174       0.0284    0.0425 
  5     0.844     0.747  0.327  0.0174       0.0283    0.0439 
  6     0.843     0.745  0.327  0.017        0.0278    0.0448 
  7     0.842     0.744  0.329  0.0174       0.0284    0.0452 

Cost was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 4. 
> 
> set.seed(857)
> rfFitCost <- train(x = trainData[, predictors],
+                    y = trainData$Class,
+                    method = "rf",
+                    metric = "Cost",
+                    maximize = FALSE,
+                    tuneLength = 10,
+                    ntree = 2000,
+                    classwt = c(VF = 1, F = 1, M = 5, L = 10),
+                    importance = TRUE,
+                    trControl = ctrl)
note: only 6 unique complexity parameters in default grid. Truncating the grid to 6 .

> rfFitCost
3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

No pre-processing
Resampling: Cross-Validation (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  mtry  Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  2     0.84      0.74   0.339  0.0165       0.027     0.0409 
  3     0.843     0.745  0.345  0.0159       0.026     0.0419 
  4     0.844     0.747  0.342  0.0176       0.0288    0.0428 
  5     0.844     0.747  0.34   0.0167       0.0274    0.0455 
  6     0.845     0.749  0.339  0.0169       0.0275    0.0436 
  7     0.845     0.749  0.336  0.0173       0.0282    0.0419 

Cost was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 7. 
> 
> c5Grid <- expand.grid(.trials = c(1, (1:10)*10),
+                       .model = "tree",
+                       .winnow = c(TRUE, FALSE))
> set.seed(857)
> c50Fit <- train(x = trainData[, predictors],
+                 y = trainData$Class,
+                 method = "C5.0",
+                 metric = "Cost",
+                 maximize = FALSE,
+                 tuneGrid = c5Grid,
+                 trControl = ctrl)
> c50Fit
3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

No pre-processing
Resampling: Cross-Validation (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  trials  winnow  Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  1       FALSE   0.801     0.677  0.397  0.0184       0.0306    0.0477 
  1       TRUE    0.801     0.678  0.397  0.0181       0.0301    0.0464 
  10      FALSE   0.832     0.728  0.339  0.019        0.0314    0.0471 
  10      TRUE    0.831     0.726  0.341  0.0185       0.0303    0.0467 
  20      FALSE   0.836     0.734  0.328  0.019        0.0309    0.0454 
  20      TRUE    0.834     0.732  0.329  0.0191       0.031     0.0479 
  30      FALSE   0.839     0.739  0.327  0.0184       0.0303    0.0452 
  30      TRUE    0.838     0.737  0.327  0.0178       0.0294    0.0476 
  40      FALSE   0.84      0.741  0.321  0.0178       0.0291    0.0453 
  40      TRUE    0.839     0.739  0.321  0.0169       0.0277    0.0475 
  50      FALSE   0.841     0.742  0.318  0.0175       0.0289    0.0423 
  50      TRUE    0.84      0.741  0.317  0.0161       0.0264    0.0441 
  60      FALSE   0.841     0.743  0.318  0.0175       0.0289    0.0447 
  60      TRUE    0.841     0.742  0.318  0.0163       0.0267    0.0457 
  70      FALSE   0.842     0.743  0.317  0.0181       0.0299    0.0458 
  70      TRUE    0.841     0.743  0.316  0.0175       0.0287    0.0474 
  80      FALSE   0.841     0.742  0.317  0.0187       0.0308    0.0483 
  80      TRUE    0.841     0.743  0.315  0.0181       0.0297    0.0493 
  90      FALSE   0.841     0.742  0.318  0.0196       0.0322    0.0478 
  90      TRUE    0.842     0.743  0.315  0.0187       0.0308    0.0493 
  100     FALSE   0.841     0.742  0.318  0.0194       0.032     0.0493 
  100     TRUE    0.841     0.743  0.315  0.0186       0.0304    0.0503 

Tuning parameter 'model' was held constant at a value of 'tree'
Cost was used to select the optimal model using  the smallest value.
The final values used for the model were model = tree, trials = 80 and winnow
 = TRUE. 
> 
> set.seed(857)
> c50Cost <- train(x = trainData[, predictors],
+                  y = trainData$Class,
+                  method = "C5.0",
+                  metric = "Cost",
+                  maximize = FALSE,
+                  costs = costMatrix,
+                  tuneGrid = c5Grid,
+                  trControl = ctrl)
> c50Cost
3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

No pre-processing
Resampling: Cross-Validation (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  trials  winnow  Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  1       FALSE   0.796     0.667  0.462  0.0186       0.0312    0.0523 
  1       TRUE    0.773     0.623  0.554  0.0372       0.07      0.129  
  10      FALSE   0.828     0.721  0.348  0.0199       0.0328    0.0478 
  10      TRUE    0.791     0.654  0.462  0.0515       0.0949    0.174  
  20      FALSE   0.836     0.734  0.329  0.0198       0.0328    0.0502 
  20      TRUE    0.797     0.664  0.445  0.0543       0.0993    0.182  
  30      FALSE   0.836     0.735  0.321  0.018        0.0296    0.0501 
  30      TRUE    0.797     0.664  0.443  0.0543       0.0994    0.183  
  40      FALSE   0.837     0.736  0.318  0.0176       0.0288    0.0465 
  40      TRUE    0.797     0.664  0.441  0.0542       0.0993    0.184  
  50      FALSE   0.837     0.736  0.316  0.017        0.0278    0.0459 
  50      TRUE    0.796     0.663  0.441  0.0537       0.0986    0.184  
  60      FALSE   0.836     0.735  0.319  0.0174       0.0285    0.0483 
  60      TRUE    0.797     0.664  0.441  0.0541       0.0992    0.184  
  70      FALSE   0.837     0.737  0.315  0.0171       0.0281    0.0469 
  70      TRUE    0.797     0.664  0.44   0.0541       0.0992    0.185  
  80      FALSE   0.837     0.736  0.317  0.0175       0.0287    0.0479 
  80      TRUE    0.797     0.665  0.438  0.0543       0.0996    0.186  
  90      FALSE   0.838     0.737  0.316  0.0183       0.03      0.0489 
  90      TRUE    0.797     0.665  0.438  0.0547       0.1       0.186  
  100     FALSE   0.839     0.739  0.319  0.0179       0.0295    0.0502 
  100     TRUE    0.798     0.667  0.44   0.0554       0.101     0.185  

Tuning parameter 'model' was held constant at a value of 'tree'
Cost was used to select the optimal model using  the smallest value.
The final values used for the model were model = tree, trials = 70 and winnow
 = FALSE. 
> 
> set.seed(857)
> bagFit <- train(x = trainData[, predictors],
+                 y = trainData$Class,
+                 method = "treebag",
+                 metric = "Cost",
+                 maximize = FALSE,
+                 nbagg = 50,
+                 trControl = ctrl)
Loading required package: prodlim
Loading required package: prodlim
Loading required package: prodlim
Loading required package: prodlim
Loading required package: prodlim
Loading required package: prodlim
Loading required package: prodlim
Loading required package: prodlim
Loading required package: prodlim
Loading required package: prodlim
Loading required package: prodlim
Loading required package: prodlim
Loading required package: prodlim
Loading required package: prodlim
KernSmooth 2.23 loaded
Copyright M. P. Wand 1997-2009
KernSmooth 2.23 loaded
Copyright M. P. Wand 1997-2009
KernSmooth 2.23 loaded
Copyright M. P. Wand 1997-2009
KernSmooth 2.23 loaded
Copyright M. P. Wand 1997-2009
KernSmooth 2.23 loaded
Copyright M. P. Wand 1997-2009
KernSmooth 2.23 loaded
Copyright M. P. Wand 1997-2009
KernSmooth 2.23 loaded
Copyright M. P. Wand 1997-2009
KernSmooth 2.23 loaded
Copyright M. P. Wand 1997-2009
KernSmooth 2.23 loaded
Copyright M. P. Wand 1997-2009
KernSmooth 2.23 loaded
Copyright M. P. Wand 1997-2009
KernSmooth 2.23 loaded
Copyright M. P. Wand 1997-2009
KernSmooth 2.23 loaded
Copyright M. P. Wand 1997-2009
KernSmooth 2.23 loaded
Copyright M. P. Wand 1997-2009
KernSmooth 2.23 loaded
Copyright M. P. Wand 1997-2009
Loading required package: prodlim
KernSmooth 2.23 loaded
Copyright M. P. Wand 1997-2009
> bagFit
3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

No pre-processing
Resampling: Cross-Validation (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results

  Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  0.836     0.734  0.331  0.0167       0.0272    0.0446 

 
> 
> ### Use the caret bag() function to bag the cost-sensitive CART model
> rpCost <- function(x, y)
+ {
+   costMatrix <- ifelse(diag(4) == 1, 0, 1)
+   costMatrix[4, 1] <- 10
+   costMatrix[3, 1] <- 5
+   costMatrix[4, 2] <- 5
+   costMatrix[3, 2] <- 5
+   library(rpart)
+   tmp <- x
+   tmp$y <- y
+   rpart(y~., data = tmp, control = rpart.control(cp = 0),
+         parms =list(loss = costMatrix))
+ }
> rpPredict <- function(object, x) predict(object, x)
> 
> rpAgg <- function (x, type = "class")
+ {
+   pooled <- x[[1]] * NA
+   n <- nrow(pooled)
+   classes <- colnames(pooled)
+   for (i in 1:ncol(pooled))
+   {
+     tmp <- lapply(x, function(y, col) y[, col], col = i)
+     tmp <- do.call("rbind", tmp)
+     pooled[, i] <- apply(tmp, 2, median)
+   }
+   pooled <- apply(pooled, 1, function(x) x/sum(x))
+   if (n != nrow(pooled)) pooled <- t(pooled)
+   out <- factor(classes[apply(pooled, 1, which.max)], levels = classes)
+   out
+ }
> 
> 
> set.seed(857)
> rpCostBag <- train(trainData[, predictors],
+                    trainData$Class,
+                    "bag",
+                    B = 50,
+                    bagControl = bagControl(fit = rpCost,
+                                            predict = rpPredict,
+                                            aggregate = rpAgg,
+                                            downSample = FALSE,
+                                            allowParallel = FALSE),
+                    trControl = ctrl)
> rpCostBag
3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

No pre-processing
Resampling: Cross-Validation (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results

  Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  0.81      0.695  0.362  0.0183       0.0295    0.0446 

Tuning parameter 'vars' was held constant at a value of 7
 
> 
> set.seed(857)
> svmRFit <- train(modForm ,
+                  data = trainData,
+                  method = "svmRadial",
+                  metric = "Cost",
+                  maximize = FALSE,
+                  preProc = c("center", "scale"),
+                  tuneLength = 15,
+                  trControl = ctrl)
> svmRFit
3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

Pre-processing: centered, scaled 
Resampling: Cross-Validation (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  C     Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  0.25  0.71      0.5    0.821  0.0175       0.0313    0.0415 
  0.5   0.752     0.584  0.638  0.0199       0.0344    0.0499 
  1     0.776     0.628  0.548  0.021        0.0354    0.0478 
  2     0.786     0.647  0.517  0.0207       0.0344    0.0467 
  4     0.792     0.66   0.479  0.0197       0.0324    0.043  
  8     0.796     0.667  0.451  0.0176       0.0287    0.0403 
  16    0.801     0.676  0.431  0.0193       0.0315    0.0405 
  32    0.801     0.677  0.417  0.0188       0.0305    0.0423 
  64    0.802     0.68   0.403  0.02         0.0326    0.0474 
  128   0.804     0.683  0.393  0.022        0.0362    0.0522 
  256   0.808     0.689  0.381  0.0209       0.0342    0.048  
  512   0.806     0.688  0.38   0.0214       0.0345    0.0513 
  1020  0.805     0.685  0.38   0.021        0.034     0.0495 
  2050  0.802     0.681  0.386  0.0194       0.0317    0.0482 
  4100  0.796     0.671  0.401  0.0204       0.033     0.047  

Tuning parameter 'sigma' was held constant at a value of 0.0451
Cost was used to select the optimal model using  the smallest value.
The final values used for the model were C = 1024 and sigma = 0.0451. 
> 
> set.seed(857)
> svmRFitCost <- train(modForm, data = trainData,
+                      method = "svmRadial",
+                      metric = "Cost",
+                      maximize = FALSE,
+                      preProc = c("center", "scale"),
+                      class.weights = c(VF = 1, F = 1, M = 5, L = 10),
+                      tuneLength = 15,
+                      trControl = ctrl)
> svmRFitCost
3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

Pre-processing: centered, scaled 
Resampling: Cross-Validation (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  C     Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  0.25  0.685     0.519  0.376  0.0231       0.0342    0.0401 
  0.5   0.715     0.561  0.353  0.0191       0.0291    0.0334 
  1     0.734     0.587  0.346  0.0195       0.0294    0.0359 
  2     0.748     0.607  0.343  0.0167       0.0256    0.0378 
  4     0.757     0.618  0.344  0.0164       0.0256    0.0334 
  8     0.766     0.631  0.347  0.016        0.025     0.0366 
  16    0.778     0.648  0.345  0.0194       0.0307    0.0455 
  32    0.782     0.654  0.351  0.0203       0.0324    0.0472 
  64    0.788     0.662  0.352  0.0223       0.0354    0.0508 
  128   0.791     0.666  0.358  0.0226       0.0361    0.0489 
  256   0.791     0.666  0.365  0.0233       0.0374    0.0498 
  512   0.795     0.672  0.368  0.0235       0.0378    0.0562 
  1020  0.796     0.673  0.375  0.0223       0.0361    0.0537 
  2050  0.794     0.67   0.384  0.0196       0.0318    0.0514 
  4100  0.792     0.665  0.395  0.022        0.0353    0.0536 

Tuning parameter 'sigma' was held constant at a value of 0.0451
Cost was used to select the optimal model using  the smallest value.
The final values used for the model were C = 2 and sigma = 0.0451. 
> 
> modelList <- list(C5.0 = c50Fit,
+                   "C5.0 (Costs)" = c50Cost,
+                   CART =rpFit,
+                   "CART (Costs)" = rpFitCost,
+                   "Bagging (Costs)" = rpCostBag,
+                   FDA = fdaFit,
+                   SVM = svmRFit,
+                   "SVM (Weights)" = svmRFitCost,
+                   PLS = plsFit,
+                   "Random Forests" = rfFit,
+                   LDA = ldaFit,
+                   "LDA (Sparse)" = sldaFit,
+                   "Neural Networks" = nnetFit,
+                   Bagging = bagFit)
> 
> 
> ################################################################################
> ### Section 17.2 Results
> 
> rs <- resamples(modelList)
> summary(rs)

Call:
summary.resamples(object = rs)


Call:
summary.resamples(object = rs)

Models: C5.0, C5.0 (Costs), CART, CART (Costs), Bagging (Costs), FDA, SVM, SVM (Weights), PLS, Random Forests, LDA, LDA (Sparse), Neural Networks, Bagging 
Number of resamples: 50 

Accuracy 
                  Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA's
C5.0            0.8017  0.8330 0.8427 0.8412  0.8505 0.8815    0
C5.0 (Costs)    0.8092  0.8237 0.8355 0.8372  0.8500 0.8736    0
CART            0.7328  0.7637 0.7723 0.7738  0.7859 0.8242    0
CART (Costs)    0.6888  0.7081 0.7201 0.7199  0.7312 0.7550    0
Bagging (Costs) 0.7752  0.7960 0.8121 0.8103  0.8232 0.8444    0
FDA             0.6686  0.7199 0.7309 0.7315  0.7457 0.7723    0
SVM             0.7514  0.7925 0.8038 0.8047  0.8228 0.8477    0
SVM (Weights)   0.7168  0.7370 0.7464 0.7484  0.7601 0.7896    0
PLS             0.7061  0.7351 0.7460 0.7478  0.7608 0.7960    0
Random Forests  0.8017  0.8329 0.8444 0.8449  0.8552 0.8818    0
LDA             0.7176  0.7389 0.7511 0.7560  0.7752 0.8132    0
LDA (Sparse)    0.7176  0.7389 0.7511 0.7560  0.7752 0.8132    0
Neural Networks 0.7428  0.7863 0.7983 0.7972  0.8137 0.8501    0
Bagging         0.8006  0.8213 0.8326 0.8358  0.8462 0.8674    0

Kappa 
                  Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA's
C5.0            0.6785  0.7267 0.7460 0.7428  0.7585 0.8083    0
C5.0 (Costs)    0.6921  0.7129 0.7321 0.7366  0.7584 0.7950    0
CART            0.5655  0.6118 0.6297 0.6314  0.6505 0.7165    0
CART (Costs)    0.5193  0.5465 0.5669 0.5649  0.5825 0.6187    0
Bagging (Costs) 0.6383  0.6722 0.6978 0.6952  0.7156 0.7527    0
FDA             0.4497  0.5381 0.5535 0.5571  0.5819 0.6308    0
SVM             0.5996  0.6658 0.6829 0.6849  0.7139 0.7538    0
SVM (Weights)   0.5573  0.5892 0.6053 0.6071  0.6263 0.6697    0
PLS             0.5080  0.5558 0.5740 0.5768  0.6010 0.6598    0
Random Forests  0.6781  0.7297 0.7481 0.7482  0.7661 0.8102    0
LDA             0.5401  0.5712 0.5931 0.6020  0.6361 0.6968    0
LDA (Sparse)    0.5401  0.5712 0.5931 0.6020  0.6361 0.6968    0
Neural Networks 0.5835  0.6537 0.6751 0.6731  0.7003 0.7591    0
Bagging         0.6747  0.7130 0.7285 0.7342  0.7509 0.7869    0

Cost 
                  Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA's
C5.0            0.2110  0.2884 0.3102 0.3147  0.3444 0.4397    0
C5.0 (Costs)    0.2334  0.2833 0.3094 0.3151  0.3453 0.4195    0
CART            0.3718  0.4761 0.5144 0.5095  0.5465 0.6580    0
CART (Costs)    0.2882  0.3220 0.3425 0.3427  0.3613 0.4236    0
Bagging (Costs) 0.2767  0.3321 0.3613 0.3619  0.3794 0.4870    0
FDA             0.4813  0.5552 0.5908 0.5983  0.6433 0.7118    0
SVM             0.2803  0.3494 0.3784 0.3796  0.3978 0.5072    0
SVM (Weights)   0.2795  0.3158 0.3362 0.3429  0.3674 0.4553    0
PLS             0.5562  0.5937 0.6297 0.6364  0.6712 0.7435    0
Random Forests  0.2572  0.2976 0.3228 0.3251  0.3446 0.4294    0
LDA             0.4150  0.4913 0.5237 0.5229  0.5632 0.6254    0
LDA (Sparse)    0.4150  0.4913 0.5237 0.5229  0.5632 0.6254    0
Neural Networks 0.3055  0.3708 0.3977 0.4019  0.4319 0.5116    0
Bagging         0.2283  0.3028 0.3295 0.3313  0.3631 0.4207    0

> 
> confusionMatrix(rpFitCost, "none")
Cross-Validated (10 fold, repeated 5 times) Confusion Matrix 

(entries are un-normalized counts)
 
          Reference
Prediction    VF     F     M     L
        VF 157.5  25.6   1.9   0.2
        F   10.0  43.1   3.3   0.2
        M    9.4  37.0  34.3   5.7
        L    0.1   2.0   1.7  14.7

> confusionMatrix(rfFit, "none") 
Cross-Validated (10 fold, repeated 5 times) Confusion Matrix 

(entries are un-normalized counts)
 
          Reference
Prediction    VF     F     M     L
        VF 164.8  18.0   1.2   0.2
        F   11.9  83.8  11.6   1.9
        M    0.2   5.5  27.4   1.8
        L    0.0   0.6   1.0  17.0

> 
> plot(bwplot(rs, metric = "Cost"))
> 
> rfPred <- predict(rfFit, testData)
> rpPred <- predict(rpFitCost, testData)
> 
> confusionMatrix(rfPred, testData$Class)
Confusion Matrix and Statistics

          Reference
Prediction  VF   F   M   L
        VF 414  45   3   0
        F   28 206  27   5
        M    0  18  71   6
        L    0   0   1  40

Overall Statistics
                                          
               Accuracy : 0.8461          
                 95% CI : (0.8202, 0.8695)
    No Information Rate : 0.5116          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.7496          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: VF Class: F Class: M Class: L
Sensitivity             0.9367   0.7658  0.69608  0.78431
Specificity             0.8863   0.8992  0.96850  0.99877
Pos Pred Value          0.8961   0.7744  0.74737  0.97561
Neg Pred Value          0.9303   0.8946  0.95969  0.98663
Prevalence              0.5116   0.3113  0.11806  0.05903
Detection Rate          0.4792   0.2384  0.08218  0.04630
Detection Prevalence    0.5347   0.3079  0.10995  0.04745
> confusionMatrix(rpPred, testData$Class)
Confusion Matrix and Statistics

          Reference
Prediction  VF   F   M   L
        VF 383  61   5   1
        F   32 106   7   2
        M   26  99  87  15
        L    1   3   3  33

Overall Statistics
                                          
               Accuracy : 0.7049          
                 95% CI : (0.6732, 0.7351)
    No Information Rate : 0.5116          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.5437          
 Mcnemar's Test P-Value : < 2.2e-16       

Statistics by Class:

                     Class: VF Class: F Class: M Class: L
Sensitivity             0.8665   0.3941   0.8529  0.64706
Specificity             0.8412   0.9311   0.8163  0.99139
Pos Pred Value          0.8511   0.7211   0.3833  0.82500
Neg Pred Value          0.8575   0.7727   0.9765  0.97816
Prevalence              0.5116   0.3113   0.1181  0.05903
Detection Rate          0.4433   0.1227   0.1007  0.03819
Detection Prevalence    0.5208   0.1701   0.2627  0.04630
> 
> 
> ################################################################################
> ### Session Information
> 
> sessionInfo()
R version 3.0.0 RC (2013-03-27 r62426)
Platform: x86_64-apple-darwin10.8.0 (64-bit)

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
 [1] parallel  grid      tools     splines   stats     graphics  grDevices
 [8] utils     datasets  methods   base     

other attached packages:
 [1] kernlab_0.9-16                   ipred_0.9-1                     
 [3] prodlim_1.3.3                    C50_0.1.0-14                    
 [5] randomForest_4.6-7               earth_3.2-3                     
 [7] plotrix_3.4-6                    plotmo_1.3-2                    
 [9] leaps_2.9                        pls_2.3-0                       
[11] nnet_7.3-6                       sparseLDA_0.1-6                 
[13] mda_0.4-2                        elasticnet_1.1                  
[15] lars_1.1                         e1071_1.6-1                     
[17] class_7.3-7                      doMC_1.3.0                      
[19] iterators_1.0.6                  caret_5.16-04                   
[21] foreach_1.4.0                    lattice_0.20-15                 
[23] tabplot_1.0                      ffbase_0.7                      
[25] ff_2.2-11                        bit_1.1-10                      
[27] Hmisc_3.10-1                     survival_2.37-4                 
[29] AppliedPredictiveModeling_1.01-1 reshape2_1.2.2                  
[31] plyr_1.8                         MASS_7.3-26                     
[33] CORElearn_0.9.41                 rpart_4.1-1                     
[35] cluster_1.14.4                  

loaded via a namespace (and not attached):
[1] codetools_0.2-8    compiler_3.0.0     KernSmooth_2.23-10 stringr_0.6.2     
> 
> q("no")
> proc.time()
     user    system   elapsed 
404117.74  45475.29  58508.28 

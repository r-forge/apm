
R version 3.0.1 (2013-05-16) -- "Good Sport"
Copyright (C) 2013 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin10.8.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ################################################################################
> ### R code from Applied Predictive Modeling (2013) by Kuhn and Johnson.
> ### Copyright 2013 Kuhn and Johnson
> ### Web Page: http://www.appliedpredictivemodeling.com
> ### Contact: Max Kuhn (mxkuhn@gmail.com) 
> ###
> ### Chapter 17: Case Study: Job Scheduling
> ###
> ### Required packages: AppliedPredictiveModeling, C50, caret, doMC (optional),
> ###                    earth, Hmisc, ipred, tabplot, kernlab, lattice, MASS,
> ###                    mda, nnet, pls, randomForest, rpart, sparseLDA, 
> ###
> ### Data used: The HPC job scheduling data in the AppliedPredictiveModeling
> ###            package.
> ###
> ### Notes: 
> ### 1) This code is provided without warranty.
> ###
> ### 2) This code should help the user reproduce the results in the
> ### text. There will be differences between this code and what is is
> ### the computing section. For example, the computing sections show
> ### how the source functions work (e.g. randomForest() or plsr()),
> ### which were not directly used when creating the book. Also, there may be 
> ### syntax differences that occur over time as packages evolve. These files 
> ### will reflect those changes.
> ###
> ### 3) In some cases, the calculations in the book were run in 
> ### parallel. The sub-processes may reset the random number seed.
> ### Your results may slightly vary.
> ###
> ################################################################################
> 
> library(AppliedPredictiveModeling)
> data(schedulingData)
> 
> ### Make a vector of predictor names
> predictors <- names(schedulingData)[!(names(schedulingData) %in% c("Class"))]
> 
> ### A few summaries and plots of the data
> library(Hmisc)
Loading required package: survival
Loading required package: splines
Hmisc library by Frank E Harrell Jr

Type library(help='Hmisc'), ?Overview, or ?Hmisc.Overview')
to see overall documentation.

NOTE:Hmisc no longer redefines [.factor to drop unused levels when
subsetting.  To get the old behavior of Hmisc type dropUnusedLevels().


Attaching package: ‘Hmisc’

The following object is masked from ‘package:survival’:

    untangle.specials

The following object is masked from ‘package:base’:

    format.pval, round.POSIXt, trunc.POSIXt, units

> describe(schedulingData)
schedulingData 

 8  Variables      4331  Observations
--------------------------------------------------------------------------------
Protocol 
      n missing  unique 
   4331       0      14 

           A   C   D  E   F   G   H   I   J K   L   M   N   O
Frequency 94 160 149 96 170 155 321 381 989 6 242 451 536 581
%          2   4   3  2   4   4   7   9  23 0   6  10  12  13
--------------------------------------------------------------------------------
Compounds 
      n missing  unique    Mean     .05     .10     .25     .50     .75     .90 
   4331       0     858   497.7      27      37      98     226     448     967 
    .95 
   2512 

lowest :    20    21    22    23    24, highest: 14087 14090 14091 14097 14103 
--------------------------------------------------------------------------------
InputFields 
      n missing  unique    Mean     .05     .10     .25     .50     .75     .90 
   4331       0    1730    1537      26      48     134     426     991    4165 
    .95 
   7594 

lowest :    10    11    12    13    14, highest: 36021 45420 45628 55920 56671 
--------------------------------------------------------------------------------
Iterations 
      n missing  unique    Mean     .05     .10     .25     .50     .75     .90 
   4331       0      11   29.24      10      20      20      20      20      50 
    .95 
    100 

           10 11 15   20 30 40  50 100 125 150 200
Frequency 272  9  2 3568  3  7 153 188   1   2 126
%           6  0  0   82  0  0   4   4   0   0   3
--------------------------------------------------------------------------------
NumPending 
      n missing  unique    Mean     .05     .10     .25     .50     .75     .90 
   4331       0     303   53.39     0.0     0.0     0.0     0.0     0.0    33.0 
    .95 
  145.5 

lowest :    0    1    2    3    4, highest: 3822 3870 3878 5547 5605 
--------------------------------------------------------------------------------
Hour 
      n missing  unique    Mean     .05     .10     .25     .50     .75     .90 
   4331       0     924   13.73   7.025   9.333  10.900  14.017  16.600  18.250 
    .95 
 19.658 

lowest :  0.01667  0.03333  0.08333  0.10000  0.11667
highest: 23.20000 23.21667 23.35000 23.80000 23.98333 
--------------------------------------------------------------------------------
Day 
      n missing  unique 
   4331       0       7 

          Mon Tue Wed Thu Fri Sat Sun
Frequency 692 900 903 720 923  32 161
%          16  21  21  17  21   1   4
--------------------------------------------------------------------------------
Class 
      n missing  unique 
   4331       0       4 

VF (2211, 51%), F (1347, 31%), M (514, 12%), L (259, 6%) 
--------------------------------------------------------------------------------
> 
> library(tabplot)
Loading required package: ffbase
Loading required package: ff
Loading required package: tools
Loading required package: bit
Attaching package bit
package:bit (c) 2008-2012 Jens Oehlschlaegel (GPL-2)
creators: bit bitwhich
coercion: as.logical as.integer as.bit as.bitwhich which
operator: ! & | xor != ==
querying: print length any all min max range sum summary
bit access: length<- [ [<- [[ [[<-
for more help type ?bit

Attaching package: ‘bit’

The following object is masked from ‘package:base’:

    xor

Attaching package ff
- getOption("fftempdir")=="/var/folders/Zf/ZfjbGEqKH2GPlbqofbYnBU+++TI/-Tmp-//RtmpZwCCTR"

- getOption("ffextension")=="ff"

- getOption("ffdrop")==TRUE

- getOption("fffinonexit")==TRUE

- getOption("ffpagesize")==65536

- getOption("ffcaching")=="mmnoflush"  -- consider "ffeachflush" if your system stalls on large writes

- getOption("ffbatchbytes")==16777216 -- consider a different value for tuning your system

- getOption("ffmaxbytes")==536870912 -- consider a different value for tuning your system


Attaching package: ‘ff’

The following object is masked from ‘package:utils’:

    write.csv, write.csv2

The following object is masked from ‘package:base’:

    is.factor, is.ordered


Attaching package: ‘ffbase’

The following object is masked from ‘package:base’:

    %in%

Loading required package: grid
> tableplot(schedulingData[, c( "Class", predictors)])
> 
> mosaicplot(table(schedulingData$Protocol, 
+                  schedulingData$Class), 
+            main = "")
> 
> library(lattice)
> xyplot(Compounds ~ InputFields|Protocol,
+        data = schedulingData,
+        scales = list(x = list(log = 10), y = list(log = 10)),
+        groups = Class,
+        xlab = "Input Fields",
+        auto.key = list(columns = 4),
+        aspect = 1,
+        as.table = TRUE)
> 
> 
> ################################################################################
> ### Section 17.1 Data Splitting and Model Strategy
> 
> ## Split the data
> 
> library(caret)
Loading required package: ggplot2

Attaching package: ‘caret’

The following object is masked from ‘package:survival’:

    cluster

> set.seed(1104)
> inTrain <- createDataPartition(schedulingData$Class, p = .8, list = FALSE)
> 
> ### There are a lot of zeros and the distribution is skewed. We add
> ### one so that we can log transform the data
> schedulingData$NumPending <- schedulingData$NumPending + 1
> 
> trainData <- schedulingData[ inTrain,]
> testData  <- schedulingData[-inTrain,]
> 
> ### Create a main effects only model formula to use
> ### repeatedly. Another formula with nonlinear effects is created
> ### below.
> modForm <- as.formula(Class ~ Protocol + log10(Compounds) +
+   log10(InputFields)+ log10(Iterations) +
+   log10(NumPending) + Hour + Day)
> 
> ### Create an expanded set of predictors with interactions. 
> 
> modForm2 <- as.formula(Class ~ (Protocol + log10(Compounds) +
+   log10(InputFields)+ log10(Iterations) +
+   log10(NumPending) + Hour + Day)^2)
> 
> 
> ### Some of these terms will not be estimable. For example, if there
> ### are no data points were a particular protocol was run on a
> ### particular day, the full interaction cannot be computed. We use
> ### model.matrix() to create the whole set of predictor columns, then
> ### remove those that are zero variance
> 
> expandedTrain <- model.matrix(modForm2, data = trainData)
> expandedTest  <- model.matrix(modForm2, data = testData)
> expandedTrain <- as.data.frame(expandedTrain)
> expandedTest  <-  as.data.frame(expandedTest)
> 
> ### Some models have issues when there is a zero variance predictor
> ### within the data of a particular class, so we used caret's
> ### checkConditionalX() function to find the offending columns and
> ### remove them
> 
> zv <- checkConditionalX(expandedTrain, trainData$Class)
> 
> ### Keep the expanded set to use for models where we must manually add
> ### more complex terms (such as logistic regression)
> 
> expandedTrain <-  expandedTrain[,-zv]
> expandedTest  <-  expandedTest[, -zv]
> 
> ### Create the cost matrix
> costMatrix <- ifelse(diag(4) == 1, 0, 1)
> costMatrix[4, 1] <- 10
> costMatrix[3, 1] <- 5
> costMatrix[4, 2] <- 5
> costMatrix[3, 2] <- 5
> rownames(costMatrix) <- colnames(costMatrix) <- levels(trainData$Class)
> 
> ### Create a cost function
> cost <- function(pred, obs)
+ {
+   isNA <- is.na(pred)
+   if(!all(isNA))
+   {
+     pred <- pred[!isNA]
+     obs <- obs[!isNA]
+     
+     cost <- ifelse(pred == obs, 0, 1)
+     if(any(pred == "VF" & obs == "L")) cost[pred == "L" & obs == "VF"] <- 10
+     if(any(pred == "F" & obs == "L")) cost[pred == "F" & obs == "L"] <- 5
+     if(any(pred == "F" & obs == "M")) cost[pred == "F" & obs == "M"] <- 5
+     if(any(pred == "VF" & obs == "M")) cost[pred == "VF" & obs == "M"] <- 5
+     out <- mean(cost)
+   } else out <- NA
+   out
+ }
> 
> ### Make a summary function that can be used with caret's train() function
> costSummary <- function (data, lev = NULL, model = NULL)
+ {
+   if (is.character(data$obs))  data$obs <- factor(data$obs, levels = lev)
+   c(postResample(data[, "pred"], data[, "obs"]),
+     Cost = cost(data[, "pred"], data[, "obs"]))
+ }
> 
> ### Create a control object for the models
> ctrl <- trainControl(method = "repeatedcv", 
+                      repeats = 5,
+                      summaryFunction = costSummary)
> 
> ### Optional: parallel processing can be used via the 'do' packages,
> ### such as doMC, doMPI etc. We used doMC (not on Windows) to speed
> ### up the computations.
> 
> ### WARNING: Be aware of how much memory is needed to parallel
> ### process. It can very quickly overwhelm the available hardware. The
> ### estimate of the median memory usage (VSIZE = total memory size) 
> ### was 3300-4100M per core although the some calculations require as  
> ### much as 3400M without parallel processing. 
> 
> library(doMC)
Loading required package: foreach
Loading required package: iterators
Loading required package: parallel
> registerDoMC(14)
> 
> ### Fit the CART model with and without costs
> 
> set.seed(857)
> rpFit <- train(x = trainData[, predictors],
+                y = trainData$Class,
+                method = "rpart",
+                metric = "Cost",
+                maximize = FALSE,
+                tuneLength = 20,
+                trControl = ctrl)
Loading required package: rpart
Loading required package: class

Attaching package: ‘e1071’

The following object is masked from ‘package:Hmisc’:

    impute

> rpFit
CART 

3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  cp       Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  0.00236  0.774     0.631  0.51   0.0193       0.0323    0.0617 
  0.00249  0.773     0.63   0.514  0.0193       0.0319    0.0591 
  0.00294  0.768     0.621  0.537  0.0176       0.0305    0.0514 
  0.00324  0.766     0.617  0.542  0.0169       0.0298    0.0521 
  0.00353  0.764     0.611  0.55   0.017        0.03      0.0491 
  0.00383  0.762     0.607  0.56   0.0182       0.0321    0.0538 
  0.00471  0.76      0.603  0.569  0.0193       0.0345    0.0607 
  0.0053   0.758     0.597  0.58   0.0183       0.0326    0.0567 
  0.00589  0.756     0.594  0.585  0.0201       0.0355    0.0591 
  0.00648  0.751     0.586  0.604  0.0205       0.036     0.059  
  0.00824  0.735     0.558  0.647  0.0184       0.0327    0.0491 
  0.00942  0.727     0.544  0.663  0.0184       0.0328    0.0476 
  0.00982  0.723     0.539  0.667  0.0181       0.0325    0.047  
  0.01     0.719     0.532  0.67   0.0175       0.0317    0.0454 
  0.0159   0.703     0.505  0.697  0.0192       0.0327    0.0518 
  0.0171   0.698     0.495  0.717  0.0179       0.032     0.0586 
  0.0183   0.693     0.482  0.755  0.0208       0.0409    0.0797 
  0.0205   0.67      0.42   0.871  0.0227       0.0493    0.0626 
  0.0383   0.652     0.376  0.969  0.0177       0.0346    0.0517 
  0.274    0.568     0.159  0.992  0.0609       0.168     0.0323 

Cost was used to select the optimal model using  the smallest value.
The final value used for the model was cp = 0.00236. 
> 
> set.seed(857)
> rpFitCost <- train(x = trainData[, predictors],
+                    y = trainData$Class,
+                    method = "rpart",
+                    metric = "Cost",
+                    maximize = FALSE,
+                    tuneLength = 20,
+                    parms =list(loss = costMatrix),
+                    trControl = ctrl)
> rpFitCost
CART 

3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  cp       Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  0.00236  0.72      0.565  0.343  0.0161       0.0248    0.0325 
  0.00249  0.718     0.562  0.344  0.0162       0.0248    0.0336 
  0.00294  0.717     0.56   0.344  0.0186       0.0277    0.0349 
  0.00324  0.717     0.56   0.345  0.0182       0.0272    0.0344 
  0.00353  0.713     0.555  0.35   0.0197       0.0293    0.0362 
  0.00383  0.707     0.545  0.358  0.0201       0.0297    0.038  
  0.00471  0.699     0.533  0.366  0.0205       0.0297    0.0386 
  0.0053   0.685     0.513  0.381  0.0196       0.0281    0.0376 
  0.00589  0.675     0.501  0.392  0.0207       0.0288    0.0378 
  0.00648  0.656     0.479  0.403  0.0372       0.0482    0.0461 
  0.00824  0.63      0.449  0.428  0.0451       0.0555    0.0476 
  0.00942  0.623     0.44   0.436  0.0574       0.0687    0.0478 
  0.00982  0.62      0.436  0.443  0.0581       0.0697    0.0457 
  0.01     0.617     0.433  0.445  0.0583       0.0699    0.0436 
  0.0159   0.53      0.324  0.507  0.0257       0.0303    0.0312 
  0.0171   0.52      0.306  0.526  0.0201       0.0223    0.0276 
  0.0183   0.521     0.305  0.527  0.0194       0.0219    0.0277 
  0.0205   0.515     0.295  0.532  0.0187       0.0231    0.0299 
  0.0383   0.503     0.275  0.546  0.0161       0.0179    0.0269 
  0.274    0.119     0      0.881  0.00104      0         0.00104

Cost was used to select the optimal model using  the smallest value.
The final value used for the model was cp = 0.00236. 
> 
> set.seed(857)
> ldaFit <- train(x = expandedTrain,
+                 y = trainData$Class,
+                 method = "lda",
+                 metric = "Cost",
+                 maximize = FALSE,
+                 trControl = ctrl)
Loading required package: MASS
> ldaFit
Linear Discriminant Analysis 

3467 samples
 112 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results

  Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  0.756     0.602  0.523  0.0232       0.0389    0.0495 

 
> 
> sldaGrid <- expand.grid(NumVars = seq(2, 112, by = 5),
+                         lambda = c(0, 0.01, .1, 1, 10))
> set.seed(857)
> sldaFit <- train(x = expandedTrain,
+                  y = trainData$Class,
+                  method = "sparseLDA",
+                  tuneGrid = sldaGrid,
+                  preProc = c("center", "scale"),
+                  metric = "Cost",
+                  maximize = FALSE,
+                  trControl = ctrl)
Loading required package: sparseLDA
Loading required package: lars
Loaded lars 1.2

Loading required package: elasticnet
Loading required package: mda
> sldaFit
Sparse Linear Discriminant Analysis 

3467 samples
 112 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

Pre-processing: centered, scaled 
Resampling: Cross-Validated (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  NumVars  lambda  Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  2        0       0.662     0.416  0.692  0.018        0.0326    0.0585 
  2        0.01    0.663     0.416  0.692  0.0179       0.0331    0.058  
  2        0.1     0.663     0.417  0.691  0.0169       0.0311    0.0573 
  2        1       0.662     0.416  0.693  0.0181       0.0327    0.0602 
  2        10      0.664     0.417  0.691  0.0164       0.0306    0.0547 
  7        0       0.681     0.457  0.707  0.0187       0.0333    0.0512 
  7        0.01    0.681     0.457  0.707  0.0187       0.0333    0.0512 
  7        0.1     0.681     0.457  0.707  0.0187       0.0333    0.0512 
  7        1       0.681     0.457  0.707  0.0188       0.0334    0.0512 
  7        10      0.681     0.457  0.707  0.0193       0.0341    0.0503 
  12       0       0.688     0.47   0.687  0.0181       0.0324    0.0526 
  12       0.01    0.688     0.47   0.687  0.0181       0.0324    0.0526 
  12       0.1     0.688     0.471  0.686  0.0182       0.0325    0.0524 
  12       1       0.688     0.47   0.687  0.018        0.0321    0.0522 
  12       10      0.687     0.469  0.689  0.0183       0.0326    0.0516 
  17       0       0.694     0.482  0.661  0.0178       0.0316    0.0516 
  17       0.01    0.694     0.483  0.661  0.0178       0.0317    0.0517 
  17       0.1     0.694     0.483  0.661  0.0181       0.032     0.0519 
  17       1       0.694     0.483  0.66   0.0176       0.0313    0.0512 
  17       10      0.693     0.482  0.662  0.0175       0.0312    0.0491 
  22       0       0.699     0.493  0.651  0.0187       0.0323    0.0487 
  22       0.01    0.699     0.493  0.651  0.0187       0.0323    0.0488 
  22       0.1     0.699     0.493  0.651  0.0187       0.0323    0.0487 
  22       1       0.699     0.493  0.651  0.0187       0.0323    0.0491 
  22       10      0.698     0.491  0.652  0.0185       0.032     0.0501 
  27       0       0.704     0.502  0.638  0.0195       0.0342    0.0578 
  27       0.01    0.704     0.503  0.637  0.0194       0.034     0.0574 
  27       0.1     0.704     0.503  0.638  0.0194       0.034     0.0578 
  27       1       0.704     0.503  0.638  0.0197       0.0345    0.0584 
  27       10      0.703     0.501  0.636  0.0199       0.0347    0.0592 
  32       0       0.712     0.518  0.626  0.0191       0.0336    0.0572 
  32       0.01    0.712     0.518  0.625  0.0191       0.0336    0.0572 
  32       0.1     0.712     0.518  0.625  0.0191       0.0336    0.0571 
  32       1       0.712     0.518  0.626  0.0191       0.0335    0.057  
  32       10      0.71      0.515  0.627  0.0193       0.0337    0.0566 
  37       0       0.721     0.536  0.611  0.0187       0.0322    0.0538 
  37       0.01    0.721     0.536  0.611  0.0187       0.0322    0.0538 
  37       0.1     0.721     0.536  0.611  0.0189       0.0324    0.0541 
  37       1       0.721     0.536  0.611  0.0187       0.0321    0.0532 
  37       10      0.717     0.529  0.615  0.0197       0.0339    0.0574 
  42       0       0.725     0.544  0.596  0.0186       0.0314    0.0508 
  42       0.01    0.725     0.544  0.596  0.0186       0.0315    0.0507 
  42       0.1     0.725     0.544  0.596  0.0185       0.0313    0.0506 
  42       1       0.725     0.544  0.595  0.0183       0.0311    0.0519 
  42       10      0.723     0.541  0.598  0.0203       0.0344    0.0522 
  47       0       0.727     0.548  0.578  0.0196       0.0325    0.0478 
  47       0.01    0.727     0.548  0.579  0.0193       0.0322    0.0486 
  47       0.1     0.727     0.548  0.579  0.0195       0.0325    0.0487 
  47       1       0.727     0.548  0.579  0.0194       0.0324    0.0491 
  47       10      0.725     0.546  0.584  0.0203       0.0336    0.0515 
  52       0       0.727     0.549  0.577  0.0206       0.0344    0.0476 
  52       0.01    0.727     0.549  0.577  0.0206       0.0344    0.0476 
  52       0.1     0.727     0.549  0.577  0.0205       0.0342    0.0475 
  52       1       0.727     0.548  0.577  0.021        0.0351    0.0483 
  52       10      0.725     0.546  0.579  0.0205       0.034     0.0495 
  57       0       0.73      0.553  0.573  0.0208       0.0348    0.0463 
  57       0.01    0.729     0.553  0.573  0.021        0.0351    0.0463 
  57       0.1     0.729     0.553  0.573  0.0209       0.035     0.0463 
  57       1       0.729     0.553  0.573  0.021        0.035     0.0455 
  57       10      0.728     0.551  0.574  0.021        0.0348    0.0474 
  62       0       0.736     0.565  0.56   0.0215       0.0359    0.0475 
  62       0.01    0.736     0.565  0.56   0.0215       0.0359    0.0475 
  62       0.1     0.736     0.565  0.56   0.0214       0.0357    0.0475 
  62       1       0.736     0.565  0.56   0.0211       0.0352    0.0475 
  62       10      0.733     0.56   0.563  0.021        0.0351    0.0485 
  67       0       0.742     0.576  0.549  0.0208       0.0344    0.0431 
  67       0.01    0.743     0.576  0.549  0.0208       0.0346    0.0432 
  67       0.1     0.743     0.576  0.549  0.0208       0.0345    0.0432 
  67       1       0.743     0.577  0.547  0.0212       0.0351    0.0449 
  67       10      0.739     0.57   0.553  0.0205       0.034     0.0452 
  72       0       0.747     0.585  0.539  0.0207       0.0346    0.0456 
  72       0.01    0.747     0.585  0.539  0.0207       0.0346    0.0456 
  72       0.1     0.747     0.585  0.539  0.0206       0.0344    0.0454 
  72       1       0.747     0.584  0.54   0.0205       0.0343    0.0447 
  72       10      0.743     0.578  0.546  0.0204       0.034     0.0432 
  77       0       0.751     0.591  0.534  0.0207       0.0347    0.042  
  77       0.01    0.751     0.591  0.534  0.0207       0.0347    0.042  
  77       0.1     0.751     0.591  0.534  0.0208       0.0348    0.0421 
  77       1       0.75      0.589  0.535  0.0213       0.0358    0.0429 
  77       10      0.747     0.584  0.54   0.0207       0.0345    0.0424 
  82       0       0.753     0.595  0.529  0.0196       0.0326    0.0409 
  82       0.01    0.753     0.595  0.529  0.0196       0.0326    0.041  
  82       0.1     0.753     0.595  0.529  0.0196       0.0326    0.0404 
  82       1       0.753     0.594  0.53   0.0199       0.0331    0.0399 
  82       10      0.748     0.586  0.537  0.0215       0.0359    0.0418 
  87       0       0.755     0.598  0.526  0.0202       0.0336    0.0428 
  87       0.01    0.755     0.598  0.526  0.0202       0.0336    0.0428 
  87       0.1     0.755     0.598  0.525  0.0203       0.0339    0.043  
  87       1       0.755     0.598  0.526  0.0202       0.0336    0.0412 
  87       10      0.75      0.59   0.532  0.0207       0.0347    0.0404 
  92       0       0.754     0.598  0.526  0.0214       0.0355    0.0451 
  92       0.01    0.754     0.598  0.527  0.0215       0.0357    0.045  
  92       0.1     0.755     0.598  0.526  0.0216       0.036     0.0452 
  92       1       0.754     0.598  0.526  0.0207       0.0345    0.0452 
  92       10      0.752     0.593  0.531  0.0213       0.0357    0.044  
  97       0       0.755     0.599  0.526  0.0217       0.0361    0.0452 
  97       0.01    0.755     0.599  0.526  0.0218       0.0363    0.0455 
  97       0.1     0.755     0.599  0.526  0.0218       0.0363    0.0455 
  97       1       0.755     0.599  0.525  0.0219       0.0363    0.0457 
  97       10      0.752     0.594  0.53   0.0217       0.0363    0.0444 
  102      0       0.754     0.598  0.527  0.0226       0.0377    0.0469 
  102      0.01    0.754     0.598  0.527  0.0224       0.0374    0.0467 
  102      0.1     0.754     0.598  0.527  0.0223       0.0373    0.0472 
  102      1       0.755     0.599  0.527  0.0224       0.0373    0.0475 
  102      10      0.753     0.595  0.53   0.0222       0.0371    0.0458 
  107      0       0.755     0.6    0.526  0.0232       0.0387    0.0497 
  107      0.01    0.755     0.6    0.526  0.0233       0.0389    0.0497 
  107      0.1     0.755     0.6    0.527  0.023        0.0383    0.0493 
  107      1       0.755     0.6    0.527  0.0225       0.0376    0.0479 
  107      10      0.753     0.597  0.53   0.0227       0.0378    0.0472 
  112      0       0.756     0.602  0.523  0.0232       0.0389    0.0495 
  112      0.01    0.756     0.602  0.523  0.0232       0.0388    0.0493 
  112      0.1     0.756     0.602  0.523  0.0232       0.0387    0.0501 
  112      1       0.756     0.601  0.524  0.0234       0.0391    0.0503 
  112      10      0.754     0.597  0.53   0.023        0.0384    0.0494 

Cost was used to select the optimal model using  the smallest value.
The final values used for the model were NumVars = 112 and lambda = 0. 
> 
> set.seed(857)
> nnetGrid <- expand.grid(decay = c(0, 0.001, 0.01, .1, .5),
+                         size = (1:10)*2 - 1)
> nnetFit <- train(modForm, 
+                  data = trainData,
+                  method = "nnet",
+                  metric = "Cost",
+                  maximize = FALSE,
+                  tuneGrid = nnetGrid,
+                  trace = FALSE,
+                  MaxNWts = 2000,
+                  maxit = 1000,
+                  preProc = c("center", "scale"),
+                  trControl = ctrl)
Loading required package: nnet
> nnetFit
Neural Network 

3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

Pre-processing: centered, scaled 
Resampling: Cross-Validated (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  decay  size  Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  0      1     0.683     0.463  0.86   0.0295       0.0512    0.164  
  0      3     0.743     0.577  0.607  0.027        0.045     0.0789 
  0      5     0.757     0.605  0.524  0.0215       0.0354    0.0697 
  0      7     0.766     0.62   0.499  0.02         0.0324    0.0622 
  0      9     0.769     0.627  0.466  0.0216       0.0354    0.0547 
  0      11    0.774     0.635  0.452  0.0217       0.0351    0.0498 
  0      13    0.774     0.636  0.454  0.0202       0.0327    0.0561 
  0      15    0.768     0.626  0.455  0.0216       0.0345    0.0487 
  0      17    0.773     0.637  0.436  0.0209       0.0326    0.0459 
  0      19    0.772     0.633  0.437  0.019        0.0298    0.0391 
  0.001  1     0.694     0.486  0.769  0.0234       0.0403    0.104  
  0.001  3     0.749     0.588  0.591  0.0241       0.0394    0.066  
  0.001  5     0.766     0.619  0.513  0.02         0.0332    0.0617 
  0.001  7     0.778     0.64   0.485  0.0228       0.0377    0.067  
  0.001  9     0.782     0.647  0.452  0.0217       0.0357    0.0552 
  0.001  11    0.779     0.643  0.445  0.0211       0.034     0.0493 
  0.001  13    0.779     0.644  0.434  0.0216       0.0359    0.0592 
  0.001  15    0.779     0.644  0.432  0.0197       0.0313    0.0499 
  0.001  17    0.78      0.648  0.419  0.0212       0.0345    0.0457 
  0.001  19    0.777     0.643  0.417  0.0263       0.0416    0.061  
  0.01   1     0.694     0.488  0.74   0.022        0.0376    0.0522 
  0.01   3     0.756     0.601  0.585  0.0203       0.0336    0.0629 
  0.01   5     0.769     0.622  0.528  0.0238       0.0391    0.0735 
  0.01   7     0.778     0.64   0.475  0.0179       0.03      0.0513 
  0.01   9     0.782     0.648  0.448  0.021        0.0335    0.0482 
  0.01   11    0.785     0.653  0.437  0.0226       0.0367    0.0512 
  0.01   13    0.784     0.652  0.438  0.0204       0.0329    0.0501 
  0.01   15    0.784     0.652  0.428  0.0197       0.0318    0.0465 
  0.01   17    0.782     0.65   0.419  0.0184       0.0292    0.0441 
  0.01   19    0.787     0.658  0.412  0.0201       0.0318    0.0477 
  0.1    1     0.693     0.485  0.765  0.0202       0.0342    0.048  
  0.1    3     0.759     0.604  0.588  0.021        0.0351    0.0566 
  0.1    5     0.778     0.637  0.502  0.0233       0.0382    0.0622 
  0.1    7     0.784     0.649  0.474  0.0229       0.0375    0.06   
  0.1    9     0.794     0.665  0.434  0.0175       0.0283    0.0435 
  0.1    11    0.791     0.662  0.436  0.0228       0.0369    0.0553 
  0.1    13    0.793     0.665  0.425  0.0196       0.0322    0.0519 
  0.1    15    0.794     0.667  0.421  0.0228       0.0369    0.0552 
  0.1    17    0.796     0.671  0.407  0.0226       0.0362    0.0472 
  0.1    19    0.799     0.676  0.398  0.0214       0.034     0.0437 
  0.5    1     0.707     0.5    0.848  0.0199       0.0351    0.0551 
  0.5    3     0.756     0.598  0.606  0.0182       0.0304    0.0572 
  0.5    5     0.776     0.634  0.524  0.0196       0.0327    0.0518 
  0.5    7     0.785     0.649  0.499  0.0185       0.0301    0.0514 
  0.5    9     0.788     0.655  0.471  0.0177       0.0294    0.053  
  0.5    11    0.793     0.664  0.449  0.0195       0.0324    0.047  
  0.5    13    0.793     0.663  0.448  0.022        0.0357    0.0509 
  0.5    15    0.796     0.668  0.429  0.0201       0.0325    0.0434 
  0.5    17    0.795     0.668  0.435  0.0227       0.0375    0.0527 
  0.5    19    0.801     0.677  0.422  0.02         0.0326    0.0492 

Cost was used to select the optimal model using  the smallest value.
The final values used for the model were size = 19 and decay = 0.1. 
> 
> set.seed(857)
> plsFit <- train(x = expandedTrain,
+                 y = trainData$Class,
+                 method = "pls",
+                 metric = "Cost",
+                 maximize = FALSE,
+                 tuneLength = 100,
+                 preProc = c("center", "scale"),
+                 trControl = ctrl)
Loading required package: pls

Attaching package: ‘pls’

The following object is masked from ‘package:caret’:

    R2

The following object is masked from ‘package:stats’:

    loadings

> plsFit
Partial Least Squares 

3467 samples
 112 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

Pre-processing: centered, scaled 
Resampling: Cross-Validated (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  ncomp  Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  1      0.645     0.352  0.998  0.0172       0.0342    0.0282 
  2      0.638     0.342  1.03   0.016        0.031     0.0264 
  3      0.646     0.357  1.02   0.0158       0.0311    0.0244 
  4      0.649     0.369  0.974  0.0162       0.0316    0.0408 
  5      0.662     0.4    0.921  0.0169       0.0319    0.0365 
  6      0.676     0.43   0.878  0.0195       0.0359    0.0485 
  7      0.677     0.434  0.853  0.0197       0.0363    0.0499 
  8      0.682     0.445  0.828  0.0203       0.0376    0.0532 
  9      0.689     0.457  0.796  0.0194       0.0358    0.0483 
  10     0.691     0.463  0.788  0.0194       0.0361    0.0515 
  11     0.692     0.467  0.776  0.0202       0.037     0.046  
  12     0.698     0.479  0.768  0.0196       0.0356    0.0496 
  13     0.7       0.484  0.761  0.0196       0.0352    0.0487 
  14     0.701     0.485  0.768  0.0196       0.0347    0.0493 
  15     0.701     0.486  0.766  0.0201       0.0362    0.051  
  16     0.704     0.492  0.761  0.0208       0.037     0.0504 
  17     0.707     0.497  0.761  0.0209       0.0376    0.0496 
  18     0.706     0.496  0.759  0.0194       0.0347    0.0527 
  19     0.707     0.498  0.756  0.0212       0.0376    0.0543 
  20     0.71      0.503  0.75   0.0186       0.0332    0.0486 
  21     0.716     0.514  0.74   0.0196       0.0347    0.052  
  22     0.719     0.519  0.734  0.0193       0.0344    0.0512 
  23     0.729     0.537  0.725  0.0184       0.0324    0.0485 
  24     0.726     0.533  0.731  0.0202       0.0355    0.0512 
  25     0.727     0.536  0.712  0.0198       0.0349    0.0489 
  26     0.727     0.536  0.711  0.0218       0.0381    0.0495 
  27     0.728     0.539  0.708  0.0205       0.0363    0.0495 
  28     0.728     0.539  0.703  0.0205       0.0361    0.0525 
  29     0.728     0.54   0.704  0.021        0.037     0.0514 
  30     0.73      0.543  0.698  0.0215       0.0378    0.0515 
  31     0.731     0.546  0.695  0.0213       0.0373    0.0499 
  32     0.732     0.547  0.693  0.0225       0.0393    0.0497 
  33     0.734     0.551  0.688  0.0216       0.0378    0.0487 
  34     0.736     0.553  0.684  0.0216       0.0377    0.0497 
  35     0.737     0.556  0.683  0.0198       0.0348    0.0464 
  36     0.739     0.559  0.677  0.0202       0.0353    0.0469 
  37     0.74      0.56   0.675  0.0217       0.0378    0.0503 
  38     0.74      0.561  0.673  0.0199       0.0345    0.049  
  39     0.742     0.564  0.669  0.0203       0.0354    0.0509 
  40     0.741     0.563  0.67   0.019        0.0333    0.0491 
  41     0.742     0.564  0.667  0.0196       0.034     0.0492 
  42     0.742     0.564  0.666  0.0197       0.0342    0.0509 
  43     0.742     0.565  0.662  0.0203       0.0352    0.0507 
  44     0.743     0.567  0.661  0.0202       0.0349    0.0499 
  45     0.743     0.567  0.658  0.0203       0.0354    0.0501 
  46     0.743     0.568  0.657  0.0205       0.0356    0.0503 
  47     0.743     0.568  0.655  0.0203       0.0352    0.0494 
  48     0.745     0.571  0.65   0.02         0.0347    0.0497 
  49     0.744     0.57   0.652  0.0201       0.0349    0.0507 
  50     0.745     0.571  0.65   0.0199       0.0344    0.0491 
  51     0.744     0.569  0.652  0.0197       0.0339    0.0495 
  52     0.744     0.57   0.65   0.0197       0.0341    0.0494 
  53     0.745     0.571  0.649  0.0207       0.0357    0.0512 
  54     0.745     0.572  0.648  0.0204       0.0351    0.0499 
  55     0.745     0.572  0.648  0.0203       0.0349    0.0507 
  56     0.745     0.572  0.647  0.0196       0.0337    0.051  
  57     0.746     0.573  0.644  0.0194       0.0332    0.0481 
  58     0.745     0.572  0.646  0.0191       0.0328    0.0487 
  59     0.745     0.573  0.645  0.0197       0.034     0.05   
  60     0.746     0.573  0.644  0.0198       0.0342    0.0504 
  61     0.746     0.574  0.642  0.0194       0.0335    0.0495 
  62     0.746     0.574  0.641  0.0201       0.0347    0.0499 
  63     0.746     0.574  0.641  0.0206       0.0355    0.0505 
  64     0.747     0.575  0.641  0.0201       0.0347    0.05   
  65     0.747     0.575  0.64   0.0206       0.0354    0.0491 
  66     0.747     0.576  0.638  0.02         0.0345    0.0492 
  67     0.747     0.576  0.639  0.0203       0.0349    0.0488 
  68     0.747     0.576  0.639  0.0202       0.0347    0.0487 
  69     0.747     0.575  0.64   0.0204       0.0351    0.0502 
  70     0.747     0.576  0.639  0.0198       0.034     0.0491 
  71     0.747     0.576  0.638  0.0201       0.0345    0.0486 
  72     0.748     0.577  0.636  0.0201       0.0346    0.05   
  73     0.748     0.577  0.637  0.0201       0.0345    0.0496 
  74     0.748     0.577  0.637  0.0205       0.0354    0.0516 
  75     0.747     0.576  0.638  0.0207       0.0357    0.0523 
  76     0.747     0.576  0.639  0.0205       0.0353    0.0511 
  77     0.747     0.576  0.639  0.0201       0.0346    0.0501 
  78     0.747     0.576  0.639  0.02         0.0345    0.0506 
  79     0.747     0.575  0.639  0.0198       0.0341    0.0491 
  80     0.747     0.575  0.64   0.0197       0.034     0.0495 
  81     0.747     0.575  0.641  0.02         0.0344    0.0494 
  82     0.747     0.575  0.641  0.0203       0.035     0.0498 
  83     0.747     0.575  0.641  0.0201       0.0347    0.0494 
  84     0.747     0.575  0.641  0.0203       0.0349    0.0496 
  85     0.747     0.575  0.641  0.0203       0.035     0.0497 
  86     0.747     0.575  0.641  0.0198       0.0341    0.0494 
  87     0.747     0.575  0.641  0.0201       0.0346    0.0499 
  88     0.747     0.575  0.641  0.0202       0.0348    0.0499 
  89     0.747     0.575  0.641  0.0203       0.0349    0.0498 
  90     0.747     0.575  0.641  0.0203       0.035     0.0499 
  91     0.747     0.575  0.64   0.0204       0.0351    0.0501 
  92     0.747     0.575  0.641  0.0204       0.035     0.0498 
  93     0.747     0.575  0.641  0.0205       0.0353    0.0499 
  94     0.747     0.575  0.641  0.0206       0.0353    0.0499 
  95     0.747     0.575  0.641  0.0206       0.0354    0.0499 
  96     0.747     0.575  0.641  0.0205       0.0352    0.0498 
  97     0.747     0.575  0.641  0.0205       0.0352    0.0498 
  98     0.747     0.575  0.641  0.0205       0.0352    0.0498 
  99     0.747     0.575  0.641  0.0205       0.0352    0.0498 
  100    0.747     0.575  0.641  0.0206       0.0353    0.0499 

Cost was used to select the optimal model using  the smallest value.
The final value used for the model was ncomp = 72. 
> 
> set.seed(857)
> fdaFit <- train(modForm, data = trainData,
+                 method = "fda",
+                 metric = "Cost",
+                 maximize = FALSE,
+                 tuneLength = 25,
+                 trControl = ctrl)
Loading required package: earth
Loading required package: plotmo
Loading required package: plotrix
> fdaFit
Flexible Discriminant Analysis 

3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  nprune  Accuracy  Kappa   Cost   Accuracy SD  Kappa SD  Cost SD
  2       0.524     0.0711  0.929  0.00646      0.021     0.0455 
  3       0.541     0.142   0.843  0.00898      0.0221    0.0368 
  4       0.61      0.298   0.79   0.0143       0.03      0.0412 
  5       0.659     0.405   0.753  0.0156       0.03      0.042  
  6       0.678     0.451   0.75   0.018        0.0324    0.0468 
  7       0.684     0.466   0.699  0.0174       0.0305    0.0513 
  8       0.693     0.487   0.64   0.0206       0.0359    0.0522 
  9       0.695     0.491   0.634  0.0214       0.0369    0.0549 
  10      0.698     0.496   0.631  0.021        0.0363    0.0551 
  11      0.71      0.518   0.62   0.0224       0.0382    0.0575 
  12      0.713     0.524   0.617  0.0204       0.0351    0.054  
  13      0.715     0.529   0.612  0.0229       0.0388    0.0584 
  14      0.724     0.544   0.602  0.0222       0.0375    0.0593 
  15      0.726     0.547   0.602  0.019        0.0328    0.0567 
  16      0.727     0.548   0.602  0.0202       0.0344    0.0559 
  17      0.725     0.545   0.608  0.019        0.033     0.0571 
  18      0.726     0.547   0.606  0.0205       0.0352    0.0588 
  19      0.727     0.548   0.607  0.0206       0.0348    0.0598 
  20      0.727     0.549   0.606  0.0208       0.0353    0.0596 
  21      0.729     0.552   0.602  0.0213       0.0358    0.0572 
  22      0.731     0.555   0.6    0.0213       0.0361    0.0583 
  23      0.732     0.557   0.598  0.0202       0.0343    0.0562 

Tuning parameter 'degree' was held constant at a value of 1
Cost was used to select the optimal model using  the smallest value.
The final values used for the model were degree = 1 and nprune = 23. 
> 
> set.seed(857)
> rfFit <- train(x = trainData[, predictors],
+                y = trainData$Class,
+                method = "rf",
+                metric = "Cost",
+                maximize = FALSE,
+                tuneLength = 10,
+                ntree = 2000,
+                importance = TRUE,
+                trControl = ctrl)
Loading required package: randomForest
randomForest 4.6-7
Type rfNews() to see new features/changes/bug fixes.

Attaching package: ‘randomForest’

The following object is masked from ‘package:Hmisc’:

    combine

note: only 6 unique complexity parameters in default grid. Truncating the grid to 6 .

> rfFit
Random Forest 

3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  mtry  Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  2     0.842     0.743  0.336  0.0168       0.0275    0.042  
  3     0.845     0.748  0.328  0.0176       0.0289    0.0419 
  4     0.845     0.748  0.326  0.0173       0.0282    0.0434 
  5     0.843     0.746  0.328  0.0166       0.0272    0.0443 
  6     0.843     0.745  0.328  0.0172       0.0282    0.0462 
  7     0.842     0.744  0.328  0.0171       0.0279    0.0437 

Cost was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 4. 
> 
> set.seed(857)
> rfFitCost <- train(x = trainData[, predictors],
+                    y = trainData$Class,
+                    method = "rf",
+                    metric = "Cost",
+                    maximize = FALSE,
+                    tuneLength = 10,
+                    ntree = 2000,
+                    classwt = c(VF = 1, F = 1, M = 5, L = 10),
+                    importance = TRUE,
+                    trControl = ctrl)
note: only 6 unique complexity parameters in default grid. Truncating the grid to 6 .

> rfFitCost
Random Forest 

3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  mtry  Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  2     0.84      0.739  0.34   0.0171       0.0281    0.0452 
  3     0.843     0.745  0.345  0.0159       0.0259    0.0413 
  4     0.844     0.746  0.345  0.016        0.0263    0.0439 
  5     0.844     0.747  0.341  0.0182       0.0298    0.0459 
  6     0.846     0.75   0.337  0.0168       0.0275    0.0432 
  7     0.845     0.748  0.337  0.0169       0.0274    0.0416 

Cost was used to select the optimal model using  the smallest value.
The final value used for the model was mtry = 7. 
> 
> c5Grid <- expand.grid(trials = c(1, (1:10)*10),
+                       model = "tree",
+                       winnow = c(TRUE, FALSE))
> set.seed(857)
> c50Fit <- train(x = trainData[, predictors],
+                 y = trainData$Class,
+                 method = "C5.0",
+                 metric = "Cost",
+                 maximize = FALSE,
+                 tuneGrid = c5Grid,
+                 trControl = ctrl)
Loading required package: C50
Loading required package: plyr

Attaching package: ‘plyr’

The following object is masked from ‘package:Hmisc’:

    is.discrete, summarize

> c50Fit
C5.0 

3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  winnow  trials  Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  FALSE   1       0.801     0.677  0.396  0.0183       0.0305    0.0476 
  FALSE   10      0.833     0.729  0.338  0.0185       0.0308    0.047  
  FALSE   20      0.836     0.735  0.326  0.0177       0.0291    0.0471 
  FALSE   30      0.839     0.739  0.324  0.0175       0.0288    0.0439 
  FALSE   40      0.839     0.739  0.324  0.0177       0.0289    0.0433 
  FALSE   50      0.839     0.739  0.322  0.0175       0.0286    0.0451 
  FALSE   60      0.839     0.74   0.322  0.0185       0.0303    0.0444 
  FALSE   70      0.84      0.741  0.32   0.0165       0.0271    0.0432 
  FALSE   80      0.84      0.741  0.319  0.0171       0.0281    0.0431 
  FALSE   90      0.841     0.743  0.318  0.0163       0.027     0.044  
  FALSE   100     0.841     0.742  0.32   0.016        0.0263    0.0432 
  TRUE    1       0.801     0.678  0.397  0.018        0.0299    0.0463 
  TRUE    10      0.832     0.727  0.34   0.0182       0.0302    0.0484 
  TRUE    20      0.834     0.732  0.327  0.0176       0.0288    0.048  
  TRUE    30      0.837     0.737  0.323  0.0168       0.0276    0.0456 
  TRUE    40      0.838     0.737  0.323  0.0167       0.0272    0.0443 
  TRUE    50      0.838     0.737  0.32   0.0164       0.0267    0.0451 
  TRUE    60      0.839     0.739  0.32   0.017        0.0276    0.0436 
  TRUE    70      0.839     0.739  0.319  0.0158       0.0258    0.0436 
  TRUE    80      0.839     0.74   0.318  0.0161       0.0264    0.0438 
  TRUE    90      0.84      0.741  0.317  0.0161       0.0265    0.0453 
  TRUE    100     0.841     0.742  0.317  0.0158       0.0259    0.0451 

Tuning parameter 'model' was held constant at a value of tree
Cost was used to select the optimal model using  the smallest value.
The final values used for the model were trials = 90, model = tree and winnow
 = TRUE. 
> 
> set.seed(857)
> c50Cost <- train(x = trainData[, predictors],
+                  y = trainData$Class,
+                  method = "C5.0",
+                  metric = "Cost",
+                  maximize = FALSE,
+                  costs = costMatrix,
+                  tuneGrid = c5Grid,
+                  trControl = ctrl)
> c50Cost
C5.0 

3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  winnow  trials  Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  FALSE   1       0.796     0.667  0.462  0.0185       0.0312    0.0526 
  FALSE   10      0.829     0.723  0.346  0.0188       0.0311    0.047  
  FALSE   20      0.834     0.731  0.33   0.0204       0.0337    0.0506 
  FALSE   30      0.835     0.733  0.325  0.0192       0.0318    0.048  
  FALSE   40      0.835     0.733  0.322  0.018        0.0297    0.0433 
  FALSE   50      0.836     0.735  0.318  0.0192       0.0316    0.0442 
  FALSE   60      0.836     0.734  0.318  0.0186       0.0307    0.045  
  FALSE   70      0.837     0.736  0.315  0.0181       0.0299    0.0454 
  FALSE   80      0.837     0.737  0.314  0.0189       0.031     0.0461 
  FALSE   90      0.839     0.739  0.314  0.0178       0.0293    0.0462 
  FALSE   100     0.839     0.74   0.317  0.0183       0.0302    0.0483 
  TRUE    1       0.773     0.624  0.554  0.0368       0.0694    0.128  
  TRUE    10      0.793     0.658  0.461  0.0511       0.094     0.174  
  TRUE    20      0.796     0.663  0.449  0.0524       0.0963    0.179  
  TRUE    30      0.797     0.664  0.446  0.0529       0.097     0.181  
  TRUE    40      0.796     0.664  0.446  0.0527       0.0967    0.181  
  TRUE    50      0.796     0.663  0.445  0.0525       0.0964    0.181  
  TRUE    60      0.796     0.663  0.444  0.0523       0.0962    0.182  
  TRUE    70      0.796     0.664  0.443  0.0522       0.096     0.182  
  TRUE    80      0.798     0.666  0.441  0.0533       0.0977    0.184  
  TRUE    90      0.799     0.668  0.441  0.0542       0.0991    0.184  
  TRUE    100     0.799     0.668  0.442  0.0542       0.0991    0.183  

Tuning parameter 'model' was held constant at a value of tree
Cost was used to select the optimal model using  the smallest value.
The final values used for the model were trials = 90, model = tree and winnow
 = FALSE. 
> 
> set.seed(857)
> bagFit <- train(x = trainData[, predictors],
+                 y = trainData$Class,
+                 method = "treebag",
+                 metric = "Cost",
+                 maximize = FALSE,
+                 nbagg = 50,
+                 trControl = ctrl)
Loading required package: ipred
Loading required package: prodlim
KernSmooth 2.23 loaded
Copyright M. P. Wand 1997-2009
> bagFit
Bagged CART 

3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results

  Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  0.836     0.735  0.333  0.0155       0.0249    0.0417 

 
> 
> ### Use the caret bag() function to bag the cost-sensitive CART model
> rpCost <- function(x, y)
+ {
+   costMatrix <- ifelse(diag(4) == 1, 0, 1)
+   costMatrix[4, 1] <- 10
+   costMatrix[3, 1] <- 5
+   costMatrix[4, 2] <- 5
+   costMatrix[3, 2] <- 5
+   library(rpart)
+   tmp <- x
+   tmp$y <- y
+   rpart(y~., data = tmp, control = rpart.control(cp = 0),
+         parms =list(loss = costMatrix))
+ }
> rpPredict <- function(object, x) predict(object, x)
> 
> rpAgg <- function (x, type = "class")
+ {
+   pooled <- x[[1]] * NA
+   n <- nrow(pooled)
+   classes <- colnames(pooled)
+   for (i in 1:ncol(pooled))
+   {
+     tmp <- lapply(x, function(y, col) y[, col], col = i)
+     tmp <- do.call("rbind", tmp)
+     pooled[, i] <- apply(tmp, 2, median)
+   }
+   pooled <- apply(pooled, 1, function(x) x/sum(x))
+   if (n != nrow(pooled)) pooled <- t(pooled)
+   out <- factor(classes[apply(pooled, 1, which.max)], levels = classes)
+   out
+ }
> 
> 
> set.seed(857)
> rpCostBag <- train(trainData[, predictors],
+                    trainData$Class,
+                    "bag",
+                    B = 50,
+                    bagControl = bagControl(fit = rpCost,
+                                            predict = rpPredict,
+                                            aggregate = rpAgg,
+                                            downSample = FALSE,
+                                            allowParallel = FALSE),
+                    trControl = ctrl)
> rpCostBag
Bagged Model 

3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results

  Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  0.807     0.689  0.369  0.0163       0.0263    0.0446 

Tuning parameter 'vars' was held constant at a value of 7
 
> 
> set.seed(857)
> svmRFit <- train(modForm ,
+                  data = trainData,
+                  method = "svmRadial",
+                  metric = "Cost",
+                  maximize = FALSE,
+                  preProc = c("center", "scale"),
+                  tuneLength = 15,
+                  trControl = ctrl)
Loading required package: kernlab
> svmRFit
Support Vector Machines with Radial Basis Function Kernel 

3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

Pre-processing: centered, scaled 
Resampling: Cross-Validated (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  C     Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  0.25  0.704     0.486  0.853  0.0168       0.0299    0.0405 
  0.5   0.744     0.568  0.671  0.0202       0.0352    0.0548 
  1     0.77      0.618  0.562  0.0193       0.0332    0.0494 
  2     0.784     0.644  0.522  0.0207       0.0347    0.0476 
  4     0.791     0.658  0.49   0.0194       0.0322    0.044  
  8     0.797     0.668  0.456  0.0181       0.0297    0.0391 
  16    0.799     0.673  0.438  0.0184       0.0299    0.0413 
  32    0.801     0.677  0.424  0.0183       0.0296    0.0394 
  64    0.802     0.679  0.415  0.0183       0.0298    0.0446 
  128   0.802     0.68   0.404  0.0202       0.0331    0.0495 
  256   0.805     0.684  0.393  0.022        0.0363    0.0522 
  512   0.807     0.689  0.385  0.021        0.0345    0.0533 
  1020  0.808     0.69   0.38   0.0212       0.0345    0.0543 
  2050  0.804     0.684  0.387  0.0218       0.0353    0.0518 
  4100  0.802     0.679  0.391  0.0199       0.0324    0.0489 

Tuning parameter 'sigma' was held constant at a value of 0.03332721
Cost was used to select the optimal model using  the smallest value.
The final values used for the model were sigma = 0.0333 and C = 1024. 
> 
> set.seed(857)
> svmRFitCost <- train(modForm, data = trainData,
+                      method = "svmRadial",
+                      metric = "Cost",
+                      maximize = FALSE,
+                      preProc = c("center", "scale"),
+                      class.weights = c(VF = 1, F = 1, M = 5, L = 10),
+                      tuneLength = 15,
+                      trControl = ctrl)
> svmRFitCost
Support Vector Machines with Radial Basis Function Kernel 

3467 samples
   7 predictors
   4 classes: 'VF', 'F', 'M', 'L' 

Pre-processing: centered, scaled 
Resampling: Cross-Validated (10 fold, repeated 5 times) 

Summary of sample sizes: 3120, 3120, 3120, 3121, 3120, 3120, ... 

Resampling results across tuning parameters:

  C     Accuracy  Kappa  Cost   Accuracy SD  Kappa SD  Cost SD
  0.25  0.681     0.513  0.378  0.0227       0.0333    0.0402 
  0.5   0.703     0.543  0.365  0.0201       0.0303    0.0354 
  1     0.726     0.576  0.347  0.0185       0.0278    0.0321 
  2     0.744     0.602  0.337  0.0179       0.0272    0.0356 
  4     0.753     0.614  0.339  0.0161       0.0244    0.0304 
  8     0.762     0.626  0.34   0.0165       0.0258    0.0395 
  16    0.77      0.637  0.347  0.0182       0.0288    0.0411 
  32    0.777     0.647  0.346  0.0186       0.0292    0.0446 
  64    0.783     0.655  0.35   0.0209       0.0331    0.0481 
  128   0.787     0.661  0.359  0.0223       0.0356    0.0517 
  256   0.79      0.665  0.36   0.0231       0.0371    0.0515 
  512   0.791     0.666  0.37   0.0235       0.0379    0.0521 
  1020  0.794     0.669  0.376  0.0222       0.0358    0.0534 
  2050  0.795     0.671  0.378  0.0224       0.0363    0.0517 
  4100  0.793     0.667  0.389  0.0202       0.0325    0.0503 

Tuning parameter 'sigma' was held constant at a value of 0.03332721
Cost was used to select the optimal model using  the smallest value.
The final values used for the model were sigma = 0.0333 and C = 2. 
> 
> modelList <- list(C5.0 = c50Fit,
+                   "C5.0 (Costs)" = c50Cost,
+                   CART =rpFit,
+                   "CART (Costs)" = rpFitCost,
+                   "Bagging (Costs)" = rpCostBag,
+                   FDA = fdaFit,
+                   SVM = svmRFit,
+                   "SVM (Weights)" = svmRFitCost,
+                   PLS = plsFit,
+                   "Random Forests" = rfFit,
+                   LDA = ldaFit,
+                   "LDA (Sparse)" = sldaFit,
+                   "Neural Networks" = nnetFit,
+                   Bagging = bagFit)
> 
> 
> ################################################################################
> ### Section 17.2 Results
> 
> rs <- resamples(modelList)
> summary(rs)

Call:
summary.resamples(object = rs)

Models: C5.0, C5.0 (Costs), CART, CART (Costs), Bagging (Costs), FDA, SVM, SVM (Weights), PLS, Random Forests, LDA, LDA (Sparse), Neural Networks, Bagging 
Number of resamples: 50 

Accuracy 
                  Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA's
C5.0            0.8040  0.8278 0.8427 0.8404  0.8473 0.8736    0
C5.0 (Costs)    0.8069  0.8249 0.8357 0.8387  0.8500 0.8757    0
CART            0.7328  0.7637 0.7723 0.7738  0.7859 0.8242    0
CART (Costs)    0.6888  0.7081 0.7201 0.7199  0.7312 0.7550    0
Bagging (Costs) 0.7637  0.7949 0.8092 0.8065  0.8173 0.8329    0
FDA             0.6686  0.7199 0.7309 0.7315  0.7457 0.7723    0
SVM             0.7579  0.7961 0.8055 0.8076  0.8202 0.8555    0
SVM (Weights)   0.7069  0.7320 0.7435 0.7444  0.7543 0.7896    0
PLS             0.7061  0.7351 0.7460 0.7478  0.7608 0.7960    0
Random Forests  0.8017  0.8324 0.8444 0.8447  0.8559 0.8844    0
LDA             0.7176  0.7389 0.7511 0.7560  0.7752 0.8132    0
LDA (Sparse)    0.7176  0.7389 0.7511 0.7560  0.7752 0.8132    0
Neural Networks 0.7522  0.7844 0.7991 0.7990  0.8143 0.8621    0
Bagging         0.8069  0.8262 0.8372 0.8361  0.8473 0.8671    0

Kappa 
                  Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA's
C5.0            0.6792  0.7208 0.7450 0.7414  0.7545 0.7951    0
C5.0 (Costs)    0.6860  0.7158 0.7361 0.7387  0.7600 0.7979    0
CART            0.5655  0.6118 0.6297 0.6314  0.6505 0.7165    0
CART (Costs)    0.5193  0.5465 0.5669 0.5649  0.5825 0.6187    0
Bagging (Costs) 0.6170  0.6694 0.6922 0.6891  0.7067 0.7339    0
FDA             0.4497  0.5381 0.5535 0.5571  0.5819 0.6308    0
SVM             0.6087  0.6739 0.6869 0.6895  0.7095 0.7655    0
SVM (Weights)   0.5428  0.5855 0.5990 0.6017  0.6151 0.6699    0
PLS             0.5080  0.5558 0.5740 0.5768  0.6010 0.6598    0
Random Forests  0.6784  0.7282 0.7477 0.7477  0.7655 0.8107    0
LDA             0.5401  0.5712 0.5931 0.6020  0.6361 0.6968    0
LDA (Sparse)    0.5401  0.5712 0.5931 0.6020  0.6361 0.6968    0
Neural Networks 0.6028  0.6512 0.6761 0.6761  0.6980 0.7765    0
Bagging         0.6830  0.7168 0.7346 0.7346  0.7533 0.7844    0

Cost 
                  Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA's
C5.0            0.2254  0.2919 0.3146 0.3172  0.3357 0.4265    0
C5.0 (Costs)    0.2283  0.2795 0.3112 0.3138  0.3472 0.4195    0
CART            0.3718  0.4761 0.5144 0.5095  0.5465 0.6580    0
CART (Costs)    0.2882  0.3220 0.3425 0.3427  0.3613 0.4236    0
Bagging (Costs) 0.2803  0.3345 0.3646 0.3693  0.3954 0.5130    0
FDA             0.4813  0.5552 0.5908 0.5983  0.6433 0.7118    0
SVM             0.2717  0.3465 0.3790 0.3802  0.4022 0.5260    0
SVM (Weights)   0.2565  0.3134 0.3309 0.3367  0.3598 0.4265    0
PLS             0.5562  0.5937 0.6297 0.6364  0.6712 0.7435    0
Random Forests  0.2543  0.2997 0.3184 0.3259  0.3429 0.4265    0
LDA             0.4150  0.4913 0.5237 0.5229  0.5632 0.6254    0
LDA (Sparse)    0.4150  0.4913 0.5237 0.5229  0.5632 0.6254    0
Neural Networks 0.3055  0.3729 0.3988 0.3981  0.4261 0.5029    0
Bagging         0.2630  0.3057 0.3261 0.3326  0.3581 0.4467    0

> 
> confusionMatrix(rpFitCost, "none")
Cross-Validated (10 fold, repeated 5 times) Confusion Matrix 

(entries are un-normalized counts)
 
          Reference
Prediction    VF     F     M     L
        VF 157.5  25.6   1.9   0.2
        F   10.0  43.1   3.3   0.2
        M    9.4  37.0  34.3   5.7
        L    0.1   2.0   1.7  14.7

> confusionMatrix(rfFit, "none") 
Cross-Validated (10 fold, repeated 5 times) Confusion Matrix 

(entries are un-normalized counts)
 
          Reference
Prediction    VF     F     M     L
        VF 164.8  17.9   1.3   0.2
        F   12.0  83.8  11.6   1.9
        M    0.2   5.5  27.3   1.8
        L    0.0   0.6   1.0  16.9

> 
> plot(bwplot(rs, metric = "Cost"))
> 
> rfPred <- predict(rfFit, testData)
> rpPred <- predict(rpFitCost, testData)
> 
> confusionMatrix(rfPred, testData$Class)
Confusion Matrix and Statistics

          Reference
Prediction  VF   F   M   L
        VF 414  45   3   0
        F   28 206  27   5
        M    0  18  71   6
        L    0   0   1  40

Overall Statistics
                                          
               Accuracy : 0.8461          
                 95% CI : (0.8202, 0.8695)
    No Information Rate : 0.5116          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.7496          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: VF Class: F Class: M Class: L
Sensitivity             0.9367   0.7658  0.69608  0.78431
Specificity             0.8863   0.8992  0.96850  0.99877
Pos Pred Value          0.8961   0.7744  0.74737  0.97561
Neg Pred Value          0.9303   0.8946  0.95969  0.98663
Prevalence              0.5116   0.3113  0.11806  0.05903
Detection Rate          0.4792   0.2384  0.08218  0.04630
Detection Prevalence    0.5347   0.3079  0.10995  0.04745
Balanced Accuracy       0.9115   0.8325  0.83229  0.89154
> confusionMatrix(rpPred, testData$Class)
Confusion Matrix and Statistics

          Reference
Prediction  VF   F   M   L
        VF 383  61   5   1
        F   32 106   7   2
        M   26  99  87  15
        L    1   3   3  33

Overall Statistics
                                          
               Accuracy : 0.7049          
                 95% CI : (0.6732, 0.7351)
    No Information Rate : 0.5116          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.5437          
 Mcnemar's Test P-Value : < 2.2e-16       

Statistics by Class:

                     Class: VF Class: F Class: M Class: L
Sensitivity             0.8665   0.3941   0.8529  0.64706
Specificity             0.8412   0.9311   0.8163  0.99139
Pos Pred Value          0.8511   0.7211   0.3833  0.82500
Neg Pred Value          0.8575   0.7727   0.9765  0.97816
Prevalence              0.5116   0.3113   0.1181  0.05903
Detection Rate          0.4433   0.1227   0.1007  0.03819
Detection Prevalence    0.5208   0.1701   0.2627  0.04630
Balanced Accuracy       0.8539   0.6626   0.8346  0.81922
> 
> 
> ################################################################################
> ### Session Information
> 
> sessionInfo()
R version 3.0.1 (2013-05-16)
Platform: x86_64-apple-darwin10.8.0 (64-bit)

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
 [1] parallel  grid      tools     splines   stats     graphics  grDevices
 [8] utils     datasets  methods   base     

other attached packages:
 [1] kernlab_0.9-18                  ipred_0.9-1                    
 [3] prodlim_1.3.7                   plyr_1.8                       
 [5] C50_0.1.0-15                    randomForest_4.6-7             
 [7] earth_3.2-6                     plotrix_3.4-7                  
 [9] plotmo_1.3-2                    pls_2.3-0                      
[11] nnet_7.3-6                      sparseLDA_0.1-6                
[13] mda_0.4-2                       elasticnet_1.1                 
[15] lars_1.2                        MASS_7.3-26                    
[17] e1071_1.6-1                     class_7.3-7                    
[19] rpart_4.1-1                     doMC_1.3.0                     
[21] iterators_1.0.6                 foreach_1.4.0                  
[23] caret_6.0-22                    ggplot2_0.9.3.1                
[25] lattice_0.20-15                 tabplot_1.0                    
[27] ffbase_0.8                      ff_2.2-11                      
[29] bit_1.1-10                      Hmisc_3.10-1.1                 
[31] survival_2.37-4                 AppliedPredictiveModeling_1.1-5

loaded via a namespace (and not attached):
 [1] car_2.0-17         cluster_1.14.4     codetools_0.2-8    colorspace_1.2-2  
 [5] compiler_3.0.1     CORElearn_0.9.41   dichromat_2.0-0    digest_0.6.3      
 [9] gtable_0.1.2       KernSmooth_2.23-10 labeling_0.1       munsell_0.4       
[13] proto_0.3-10       RColorBrewer_1.0-5 reshape2_1.2.2     scales_0.2.3      
[17] stringr_0.6.2     
> 
> q("no")
> proc.time()
     user    system   elapsed 
492217.97  31824.96  39801.06 
